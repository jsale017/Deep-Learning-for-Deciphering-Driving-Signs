{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","collapsed_sections":["zMc_SdyFrsIu"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Deep Learning for Deciphering Traffic Signs\n","_________________________________________________________________________________________________________________________________________________________________________________\n","\n","##### Contributors:\n"," Victor Floriano, Yifan Fan, Jose Salerno"],"metadata":{"id":"OKlU1qk4Zly7"}},{"cell_type":"markdown","source":["## Problem Statement & Motivation\n","As the world advances towards autonomous vehicles, our team has observed the remarkable efforts of large car manufacturers, who are working with data scientists to develop fully autonomous cars. Our team is excited to contribute to the development of this technology by creating a neural network model that will be able to classify different traffic signs. Our ultimate goal is to assist car makers in overcoming the challenges they may face in implementing neural network models that effectively read traffic signs and further their efforts toward a fully autonomous car or assisted driving. We believe autonomous driving to be an important problem to solve due to the great economic benefits it can generate for car manufacturers and the improvement of general driving safety.\n","\n","## Data Preparation\n"," We've selected the German Traffic Sign Recognition Benchmark (GTSRB) as our primary dataset. It's renowned for its complexity, featuring over 50,000 images across more than 40 classes of traffic signs. The GTSRB is publicly accessible through two resources. To efficiently manage the extensive and complex GTSRB dataset, our strategy integrates preprocessing for uniformity, data augmentation for robustness, and batch processing for computational efficiency. We'll employ distributed computing to parallelize operations, enhancing processing speed, and use stratified sampling for quick experimentation without compromising representativeness.\n","\n","\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"2iFnzTGUdjrz"}},{"cell_type":"markdown","source":["# Project Breakdown Summary\n","1. Data Cleaning and Transformation\n","3. MLP Models\n","4. CNN Model\n","5. Pre-trained Model\n","6. SVM\n","7. Results\n","8. Future Work\n","9. Conclusions"],"metadata":{"id":"0oACpv-ndpXW"}},{"cell_type":"markdown","source":["# 1. Data Cleaning and Transformation"],"metadata":{"id":"gSMk02Bq8CVQ"}},{"cell_type":"markdown","source":["## Importing Packages"],"metadata":{"id":"JXPh9xcKdsh3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaPgqKKKZlaG"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gvoyQffZ2-HM","outputId":"edda3c54-107e-4f02-a229-64d4c675d264","executionInfo":{"status":"ok","timestamp":1712841072127,"user_tz":240,"elapsed":1002,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Data Transformation\n","train_datagenerator = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    height_shift_range=0.2,\n","    validation_split=0.2\n",")\n","\n","#Change path as needed:\n","#Jose path: '/content/drive/MyDrive/BA865 - Group Project/GTSRBkaggle/Train'\n","#Victor path: '/content/drive/MyDrive/BU_MSBA/BA865 - Neural Networks/BA865 - Group Project/GTSRBkaggle/Train'\n","\n","train_data = '/content/drive/MyDrive/BU_MSBA/BA865 - Neural Networks/BA865 - Group Project/GTSRBkaggle/Train'"],"metadata":{"id":"TjZBv1zw2_xN","executionInfo":{"status":"ok","timestamp":1712841076983,"user_tz":240,"elapsed":3267,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Increased the batch size to speed up training\n","train_generator = train_datagenerator.flow_from_directory(\n","    train_data,\n","    target_size=(32, 32),\n","    batch_size=250,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","\n","validation_generator = train_datagenerator.flow_from_directory(\n","    train_data,\n","    target_size=(32, 32),\n","    batch_size=250,\n","    class_mode='categorical',\n","    subset='validation'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTvmTS6RQB5T","outputId":"15483898-cda5-450b-a5ed-79c1458c27cc","executionInfo":{"status":"ok","timestamp":1712841078898,"user_tz":240,"elapsed":1919,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 31376 images belonging to 43 classes.\n","Found 7843 images belonging to 43 classes.\n"]}]},{"cell_type":"code","source":["# classes = { 0:'Speed limit (20km/h)',\n","#             1:'Speed limit (30km/h)',\n","#             2:'Speed limit (50km/h)',\n","#             3:'Speed limit (60km/h)',\n","#             4:'Speed limit (70km/h)',\n","#             5:'Speed limit (80km/h)',\n","#             6:'End of speed limit (80km/h)',\n","#             7:'Speed limit (100km/h)',\n","#             8:'Speed limit (120km/h)',\n","#             9:'No passing',\n","#             10:'No passing veh over 3.5 tons',\n","#             11:'Right-of-way at intersection',\n","#             12:'Priority road',\n","#             13:'Yield',\n","#             14:'Stop',\n","#             15:'No vehicles',\n","#             16:'Veh > 3.5 tons prohibited',\n","#             17:'No entry',\n","#             18:'General caution',\n","#             19:'Dangerous curve left',\n","#             20:'Dangerous curve right',\n","#             21:'Double curve',\n","#             22:'Bumpy road',\n","#             23:'Slippery road',\n","#             24:'Road narrows on the right',\n","#             25:'Road work',\n","#             26:'Traffic signals',\n","#             27:'Pedestrians',\n","#             28:'Children crossing',\n","#             29:'Bicycles crossing',\n","#             30:'Beware of ice/snow',\n","#             31:'Wild animals crossing',\n","#             32:'End speed + passing limits',\n","#             33:'Turn right ahead',\n","#             34:'Turn left ahead',\n","#             35:'Ahead only',\n","#             36:'Go straight or right',\n","#             37:'Go straight or left',\n","#             38:'Keep right',\n","#             39:'Keep left',\n","#             40:'Roundabout mandatory',\n","#             41:'End of no passing',\n","#             42:'End no passing veh > 3.5 tons' }"],"metadata":{"id":"TA87HnFKA6w1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizing Data"],"metadata":{"id":"1Nkdw0AbTfQ9"}},{"cell_type":"code","source":["train_generator.labels.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mkm3YQoqaMKC","outputId":"3693af07-5d1c-4ef8-c2bf-e8491ddb1f8d","executionInfo":{"status":"ok","timestamp":1712841080928,"user_tz":240,"elapsed":269,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31376,)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["validation_generator.labels.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s851HtfIah97","outputId":"99231dea-9059-48a9-8eb6-0a77ec8f638c","executionInfo":{"status":"ok","timestamp":1712841081886,"user_tz":240,"elapsed":411,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7843,)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["train_generator.class_mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"JIKXJoNBao6U","outputId":"eeb17cf1-ec10-4200-eee3-878068942a91","executionInfo":{"status":"ok","timestamp":1712841081886,"user_tz":240,"elapsed":6,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'categorical'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["validation_generator.class_mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"QI04H9JParc0","outputId":"55d00c56-0afd-467b-8e9c-40b3a546dad9","executionInfo":{"status":"ok","timestamp":1712841083494,"user_tz":240,"elapsed":6,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'categorical'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train_generator.image_shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mo2xpRxEavvN","outputId":"84cdc8c0-a027-4180-8c2d-0aacbb0aab3d","executionInfo":{"status":"ok","timestamp":1712841084222,"user_tz":240,"elapsed":3,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32, 32, 3)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["validation_generator.image_shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FB_ttgxNaxxM","outputId":"76cb8855-c8b3-4373-a952-ee97a141b599","executionInfo":{"status":"ok","timestamp":1712841085364,"user_tz":240,"elapsed":270,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(32, 32, 3)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# 2. KERAS Sequential MLP\n"],"metadata":{"id":"Lhry9DmjA2Jj"}},{"cell_type":"markdown","source":["#### Sequential Model"],"metadata":{"id":"zMc_SdyFrsIu"}},{"cell_type":"code","source":["from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","\n","def MLP():\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(32, 32, 3)))\n","    model.add(Dense(256, activation='relu',))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(43, activation='softmax'))\n","    return model"],"metadata":{"id":"jNp37bONVBsG","executionInfo":{"status":"ok","timestamp":1712840026445,"user_tz":240,"elapsed":4,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model = MLP()\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgYTJpyhSPnQ","outputId":"2dfa237f-b270-45bb-8cc0-313d1f44e852","executionInfo":{"status":"ok","timestamp":1712840028501,"user_tz":240,"elapsed":12,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_1 (Flatten)         (None, 3072)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 256)               786688    \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_5 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dense_6 (Dense)             (None, 43)                2795      \n","                                                                 \n","=================================================================\n","Total params: 830635 (3.17 MB)\n","Trainable params: 830635 (3.17 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"5vaS8n1NSxLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","callback = EarlyStopping(monitor='val_loss', patience=3)\n","model.fit(train_generator, validation_data = validation_generator, epochs = 3, callbacks = [callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vptqybDBTW6T","outputId":"920a4a7b-447b-409b-b9d4-df13d3193032"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","981/981 [==============================] - 3521s 4s/step - loss: 3.4247 - accuracy: 0.0774 - val_loss: 3.1777 - val_accuracy: 0.1258\n","Epoch 2/3\n","281/981 [=======>......................] - ETA: 1:34 - loss: 3.1697 - accuracy: 0.1098"]}]},{"cell_type":"markdown","source":["## Sequential Model (Light)"],"metadata":{"id":"ZX1_i4YILOsT"}},{"cell_type":"code","source":["import tensorflow as tf\n","#Check if tensorflow recognizes the gpu available - we have a cell doing this earlier\n","#with pytorch, but we use keras for the models\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtxH9cHlO7GP","executionInfo":{"status":"ok","timestamp":1712841088864,"user_tz":240,"elapsed":243,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}},"outputId":"644b4f2a-8f6e-4b53-d1f5-11f4ac25db8b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  1\n"]}]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","\n","def MLP():\n","    model = Sequential([\n","        Flatten(input_shape=(32, 32, 3)),\n","        Dense(15, activation='relu'),\n","        Dropout(0.5),\n","        Dense(15, activation='relu'),\n","        Dropout(0.5),\n","        Dense(43, activation='softmax')\n","    ])\n","    return model\n","\n","model = MLP()\n","model.summary()\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n"],"metadata":{"id":"yfM5GuA3MREs","executionInfo":{"status":"ok","timestamp":1712841101387,"user_tz":240,"elapsed":255,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","callback = EarlyStopping(monitor='val_loss', patience=5)\n","model.fit(train_generator, validation_data = validation_generator, epochs = 100, callbacks = [callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMz7683_ORUO","outputId":"b63a0f1a-991c-4dc1-ec6a-5ac70becfbaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","  8/126 [>.............................] - ETA: 4:21:25 - loss: 3.7849 - accuracy: 0.0325"]}]},{"cell_type":"markdown","source":["## Sources:\n","\n","- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"],"metadata":{"id":"63E-98XlYngo"}}]}