{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Deciphering Traffic Signs\n",
    "# CNN Notebook\n",
    "_________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "##### Contributors:\n",
    " Victor Floriano, Yifan Fan, Jose Salerno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement & Motivation\n",
    "As the world advances towards autonomous vehicles, our team has observed the remarkable efforts of large car manufacturers, who are working with data scientists to develop fully autonomous cars. Our team is excited to contribute to the development of this technology by creating a neural network model that will be able to classify different traffic signs. Our ultimate goal is to assist car makers in overcoming the challenges they may face in implementing neural network models that effectively read traffic signs and further their efforts toward a fully autonomous car or assisted driving. We believe autonomous driving to be an important problem to solve due to the great economic benefits it can generate for car manufacturers and the improvement of general driving safety.\n",
    "\n",
    "## Data Preparation\n",
    " We've selected the German Traffic Sign Recognition Benchmark (GTSRB) as our primary dataset. It's renowned for its complexity, featuring over 50,000 images across more than 40 classes of traffic signs. The GTSRB is publicly accessible through two resources. To efficiently manage the extensive and complex GTSRB dataset, our strategy integrates preprocessing for uniformity, data augmentation for robustness, and batch processing for computational efficiency. We'll employ distributed computing to parallelize operations, enhancing processing speed, and use stratified sampling for quick experimentation without compromising representativeness.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - Models \n",
    "\n",
    " The provided code outlines the construction and operation of a convolutional neural network (CNN) designed for image classification, implemented using PyTorch. The process starts with data preprocessing, where image data undergoes resizing, tensor conversion, and normalization of the training dataset.\n",
    "\n",
    "The CNN architecture comprises multiple layers such as batch normalization and ReLU activation functions, dropout layers, and max pooling layers. Following feature extraction, the network transitions to a classifier linear layer to create predictions across 43 classes.\n",
    "\n",
    "________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "Results: \n",
    "\n",
    "- Model 1: Validation Accuracy = 98.99%\n",
    "- **Model 2: Validation Accuracy = 99.07% (Best Model)**\n",
    "- Model 3: Validation Accuracy = 97.65%\n",
    "- Model 4: Validation Accuracy = 87.41%\n",
    "- Model 5: Validation Accuracy = 89.38%\n",
    "- Model 6: Validation Accuracy = 90.38%\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 17:41:53.734069: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 17:41:53.779948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-19 17:41:55.930490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!find Train -name .ipynb_checkpoints -exec rm -r {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_wandb = True\n",
    "use_gpu = True\n",
    "gpu_available = torch.cuda.is_available()\n",
    "gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevafan12\u001b[0m (\u001b[33mevafan123\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "if enable_wandb:\n",
    "  !pip install wandb -qU\n",
    "  import wandb\n",
    "  wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1_mRxBrBq98"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00': 210, '01': 2220, '02': 2250, '03': 1410, '04': 1980, '05': 1860, '06': 420, '07': 1440, '08': 1410, '09': 1470, '10': 2010, '11': 1320, '12': 2100, '13': 2160, '14': 780, '15': 630, '16': 420, '17': 1110, '18': 1200, '19': 210, '20': 360, '21': 330, '22': 390, '23': 510, '24': 270, '25': 1500, '26': 600, '27': 240, '28': 540, '29': 270, '30': 450, '31': 780, '32': 240, '33': 689, '34': 420, '35': 1200, '36': 390, '37': 210, '38': 2070, '39': 300, '40': 360, '41': 240, '42': 240}\n",
      "Total images across all categories for training: 39209\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_per_category(directory):\n",
    "    category_image_count = {}\n",
    "\n",
    "    for category in os.listdir(directory):\n",
    "        category_path = os.path.join(directory, category)\n",
    "        \n",
    "        if os.path.isdir(category_path):\n",
    "            num_images = len([item for item in os.listdir(category_path) if item.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
    "            category_image_count[category] = num_images\n",
    "\n",
    "    return category_image_count\n",
    "\n",
    "dataset_directory = 'Train'\n",
    "image_counts = count_images_per_category(dataset_directory)\n",
    "sorted_train = {key: image_counts[key] for key in sorted(image_counts.keys(), key=lambda x: int(x))}\n",
    "\n",
    "print(sorted_train)\n",
    "total_train = sum(sorted_train.values())\n",
    "print(f\"Total images across all categories for training: {total_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 60, '1': 720, '2': 750, '3': 450, '4': 660, '5': 630, '6': 150, '7': 450, '8': 450, '9': 480, '10': 660, '11': 420, '12': 690, '13': 720, '14': 270, '15': 210, '16': 150, '17': 360, '18': 390, '19': 60, '20': 90, '21': 90, '22': 120, '23': 150, '24': 90, '25': 480, '26': 180, '27': 60, '28': 150, '29': 90, '30': 150, '31': 270, '32': 60, '33': 210, '34': 120, '35': 390, '36': 120, '37': 60, '38': 690, '39': 90, '40': 90, '41': 60, '42': 90}\n",
      "Total images across all categories for testing: 12630\n"
     ]
    }
   ],
   "source": [
    "dataset_directory = 'Test_organized'\n",
    "image_counts = count_images_per_category(dataset_directory)\n",
    "sorted_test = {key: image_counts[key] for key in sorted(image_counts.keys(), key=lambda x: int(x))}\n",
    "\n",
    "print(sorted_test)\n",
    "total_test = sum(sorted_test.values())\n",
    "print(f\"Total images across all categories for testing: {total_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), (0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "\n",
    "dataset_train = ImageFolder(\n",
    "    'Train',\n",
    "    transform = train_transforms\n",
    ")\n",
    "dataset_test = ImageFolder(\n",
    "    'Test_organized',\n",
    "    transform = train_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vC9D4yTsTbVH"
   },
   "outputs": [],
   "source": [
    "train_data, val_data = random_split(dataset_train, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8My4yFsBugm"
   },
   "source": [
    "### CNN Model - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epochs = 100\n",
    "- Batch size = 64\n",
    "- Learning Rates = 0.001\n",
    "- No Data Agumentation\n",
    "- No Class Balancing \n",
    "- Early Stopping Patience 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/evafan/wandb/run-20240417_135058-ezmhd20p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/ezmhd20p' target=\"_blank\">experiment_v1</a></strong> to <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/ezmhd20p' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/ezmhd20p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/evafan123/Project%20CNN_v1/runs/ezmhd20p?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x150cbf0329e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Project CNN_v1\", name = 'experiment_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset_test, shuffle=True, batch_size = 64, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EL-scXGS6Ocj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Define feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.3),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout2d(0.5)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 2 * 2, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1000, out_features=256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through feature extractor and classifier\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the features\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jEd8x08Iasw",
    "outputId": "c9bfeaea-9c33-4d8c-8f6c-e140ed1f7e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         Dropout2d-4           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-5             [-1, 64, 8, 8]               0\n",
      "            Conv2d-6            [-1, 192, 8, 8]         110,784\n",
      "       BatchNorm2d-7            [-1, 192, 8, 8]             384\n",
      "              ReLU-8            [-1, 192, 8, 8]               0\n",
      "         Dropout2d-9            [-1, 192, 8, 8]               0\n",
      "        MaxPool2d-10            [-1, 192, 4, 4]               0\n",
      "           Conv2d-11            [-1, 384, 4, 4]         663,936\n",
      "      BatchNorm2d-12            [-1, 384, 4, 4]             768\n",
      "             ReLU-13            [-1, 384, 4, 4]               0\n",
      "        Dropout2d-14            [-1, 384, 4, 4]               0\n",
      "           Conv2d-15            [-1, 256, 4, 4]         884,992\n",
      "      BatchNorm2d-16            [-1, 256, 4, 4]             512\n",
      "             ReLU-17            [-1, 256, 4, 4]               0\n",
      "           Conv2d-18            [-1, 256, 4, 4]         590,080\n",
      "      BatchNorm2d-19            [-1, 256, 4, 4]             512\n",
      "             ReLU-20            [-1, 256, 4, 4]               0\n",
      "        MaxPool2d-21            [-1, 256, 2, 2]               0\n",
      "        Dropout2d-22            [-1, 256, 2, 2]               0\n",
      "          Dropout-23                 [-1, 1024]               0\n",
      "           Linear-24                 [-1, 1000]       1,025,000\n",
      "             ReLU-25                 [-1, 1000]               0\n",
      "          Dropout-26                 [-1, 1000]               0\n",
      "           Linear-27                  [-1, 256]         256,256\n",
      "             ReLU-28                  [-1, 256]               0\n",
      "           Linear-29                   [-1, 43]          11,051\n",
      "================================================================\n",
      "Total params: 3,546,195\n",
      "Trainable params: 3,546,195\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.36\n",
      "Params size (MB): 13.53\n",
      "Estimated Total Size (MB): 14.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net(num_classes=43)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "summary(net,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 64\n",
    "})\n",
    "wandb.watch(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UbY4EQAuMcZp"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def early_stop(self, validation_loss, model):\n",
    "        if validation_loss < self.min_validation_loss - self.delta:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            self.best_model = model.state_dict()\n",
    "        elif validation_loss >= self.min_validation_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4nYeXugSSTCA"
   },
   "outputs": [],
   "source": [
    "def get_loss(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()  # Sum up batch loss\n",
    "\n",
    "    average_loss = total_loss / len(loader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BZ6KLxmTSYJk"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cu1wauFWRqhM"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopper(patience=5,delta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVXEYoxVRqeC",
    "outputId": "0409de5a-6998-4b27-ba0d-562376610db9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.740623712539673\n",
      "Epoch 1  batch 101 . Training Loss:  3.2844128608703613\n",
      "Epoch 1  batch 201 . Training Loss:  2.627850294113159\n",
      "Epoch 1  batch 301 . Training Loss:  2.0726101398468018\n",
      "Epoch 1  batch 401 . Training Loss:  2.0345070362091064\n",
      "Epoch 1: Validation Loss: 1.6195, Validation Accuracy: 42.10%\n",
      "Epoch 2  batch 1 . Training Loss:  1.851232886314392\n",
      "Epoch 2  batch 101 . Training Loss:  1.8435171842575073\n",
      "Epoch 2  batch 201 . Training Loss:  1.7504080533981323\n",
      "Epoch 2  batch 301 . Training Loss:  1.6081430912017822\n",
      "Epoch 2  batch 401 . Training Loss:  1.6278976202011108\n",
      "Epoch 2: Validation Loss: 1.2251, Validation Accuracy: 56.70%\n",
      "Epoch 3  batch 1 . Training Loss:  1.5973659753799438\n",
      "Epoch 3  batch 101 . Training Loss:  1.3043276071548462\n",
      "Epoch 3  batch 201 . Training Loss:  1.249068260192871\n",
      "Epoch 3  batch 301 . Training Loss:  1.0592491626739502\n",
      "Epoch 3  batch 401 . Training Loss:  0.9640299677848816\n",
      "Epoch 3: Validation Loss: 0.7157, Validation Accuracy: 73.50%\n",
      "Epoch 4  batch 1 . Training Loss:  1.3602714538574219\n",
      "Epoch 4  batch 101 . Training Loss:  0.7825140953063965\n",
      "Epoch 4  batch 201 . Training Loss:  0.9009556174278259\n",
      "Epoch 4  batch 301 . Training Loss:  0.8996531963348389\n",
      "Epoch 4  batch 401 . Training Loss:  0.9044365882873535\n",
      "Epoch 4: Validation Loss: 0.3954, Validation Accuracy: 86.94%\n",
      "Epoch 5  batch 1 . Training Loss:  0.912706732749939\n",
      "Epoch 5  batch 101 . Training Loss:  0.5289360284805298\n",
      "Epoch 5  batch 201 . Training Loss:  0.6213037371635437\n",
      "Epoch 5  batch 301 . Training Loss:  0.5647568106651306\n",
      "Epoch 5  batch 401 . Training Loss:  0.44137299060821533\n",
      "Epoch 5: Validation Loss: 0.2777, Validation Accuracy: 90.32%\n",
      "Epoch 6  batch 1 . Training Loss:  0.49486085772514343\n",
      "Epoch 6  batch 101 . Training Loss:  0.41668444871902466\n",
      "Epoch 6  batch 201 . Training Loss:  0.2856026291847229\n",
      "Epoch 6  batch 301 . Training Loss:  0.291103720664978\n",
      "Epoch 6  batch 401 . Training Loss:  0.4116847515106201\n",
      "Epoch 6: Validation Loss: 0.2061, Validation Accuracy: 92.96%\n",
      "Epoch 7  batch 1 . Training Loss:  0.5296012759208679\n",
      "Epoch 7  batch 101 . Training Loss:  0.5151572823524475\n",
      "Epoch 7  batch 201 . Training Loss:  0.41430017352104187\n",
      "Epoch 7  batch 301 . Training Loss:  0.1536451131105423\n",
      "Epoch 7  batch 401 . Training Loss:  0.17430225014686584\n",
      "Epoch 7: Validation Loss: 0.1356, Validation Accuracy: 95.57%\n",
      "Epoch 8  batch 1 . Training Loss:  0.2599322199821472\n",
      "Epoch 8  batch 101 . Training Loss:  0.3348589837551117\n",
      "Epoch 8  batch 201 . Training Loss:  0.26504069566726685\n",
      "Epoch 8  batch 301 . Training Loss:  0.16440162062644958\n",
      "Epoch 8  batch 401 . Training Loss:  0.12172074615955353\n",
      "Epoch 8: Validation Loss: 0.1275, Validation Accuracy: 95.86%\n",
      "Epoch 9  batch 1 . Training Loss:  0.17489105463027954\n",
      "Epoch 9  batch 101 . Training Loss:  0.30732041597366333\n",
      "Epoch 9  batch 201 . Training Loss:  0.33956119418144226\n",
      "Epoch 9  batch 301 . Training Loss:  0.2623386085033417\n",
      "Epoch 9  batch 401 . Training Loss:  0.24358808994293213\n",
      "Epoch 9: Validation Loss: 0.1042, Validation Accuracy: 96.93%\n",
      "Epoch 10  batch 1 . Training Loss:  0.23368699848651886\n",
      "Epoch 10  batch 101 . Training Loss:  0.11742454767227173\n",
      "Epoch 10  batch 201 . Training Loss:  0.21164977550506592\n",
      "Epoch 10  batch 301 . Training Loss:  0.19573433697223663\n",
      "Epoch 10  batch 401 . Training Loss:  0.2677750289440155\n",
      "Epoch 10: Validation Loss: 0.0861, Validation Accuracy: 97.25%\n",
      "Epoch 11  batch 1 . Training Loss:  0.236814484000206\n",
      "Epoch 11  batch 101 . Training Loss:  0.09122662991285324\n",
      "Epoch 11  batch 201 . Training Loss:  0.18173843622207642\n",
      "Epoch 11  batch 301 . Training Loss:  0.22519318759441376\n",
      "Epoch 11  batch 401 . Training Loss:  0.3123587667942047\n",
      "Epoch 11: Validation Loss: 0.0739, Validation Accuracy: 97.62%\n",
      "Epoch 12  batch 1 . Training Loss:  0.1137162521481514\n",
      "Epoch 12  batch 101 . Training Loss:  0.11212529242038727\n",
      "Epoch 12  batch 201 . Training Loss:  0.1513146460056305\n",
      "Epoch 12  batch 301 . Training Loss:  0.2142331451177597\n",
      "Epoch 12  batch 401 . Training Loss:  0.19531702995300293\n",
      "Epoch 12: Validation Loss: 0.0699, Validation Accuracy: 97.88%\n",
      "Epoch 13  batch 1 . Training Loss:  0.22379447519779205\n",
      "Epoch 13  batch 101 . Training Loss:  0.17838187515735626\n",
      "Epoch 13  batch 201 . Training Loss:  0.14223594963550568\n",
      "Epoch 13  batch 301 . Training Loss:  0.4042801856994629\n",
      "Epoch 13  batch 401 . Training Loss:  0.12145569920539856\n",
      "Epoch 13: Validation Loss: 0.0590, Validation Accuracy: 98.01%\n",
      "Epoch 14  batch 1 . Training Loss:  0.2132910192012787\n",
      "Epoch 14  batch 101 . Training Loss:  0.15257902443408966\n",
      "Epoch 14  batch 201 . Training Loss:  0.19105982780456543\n",
      "Epoch 14  batch 301 . Training Loss:  0.16089236736297607\n",
      "Epoch 14  batch 401 . Training Loss:  0.10597237944602966\n",
      "Epoch 14: Validation Loss: 0.0605, Validation Accuracy: 98.21%\n",
      "Epoch 15  batch 1 . Training Loss:  0.11342140287160873\n",
      "Epoch 15  batch 101 . Training Loss:  0.17036910355091095\n",
      "Epoch 15  batch 201 . Training Loss:  0.13905417919158936\n",
      "Epoch 15  batch 301 . Training Loss:  0.16995052993297577\n",
      "Epoch 15  batch 401 . Training Loss:  0.13196086883544922\n",
      "Epoch 15: Validation Loss: 0.0641, Validation Accuracy: 97.84%\n",
      "Epoch 16  batch 1 . Training Loss:  0.17418994009494781\n",
      "Epoch 16  batch 101 . Training Loss:  0.11783049255609512\n",
      "Epoch 16  batch 201 . Training Loss:  0.2283453643321991\n",
      "Epoch 16  batch 301 . Training Loss:  0.05818449333310127\n",
      "Epoch 16  batch 401 . Training Loss:  0.13690952956676483\n",
      "Epoch 16: Validation Loss: 0.0673, Validation Accuracy: 97.93%\n",
      "Epoch 17  batch 1 . Training Loss:  0.11086854338645935\n",
      "Epoch 17  batch 101 . Training Loss:  0.1599770039319992\n",
      "Epoch 17  batch 201 . Training Loss:  0.14644019305706024\n",
      "Epoch 17  batch 301 . Training Loss:  0.18640626966953278\n",
      "Epoch 17  batch 401 . Training Loss:  0.07428416609764099\n",
      "Epoch 17: Validation Loss: 0.0560, Validation Accuracy: 98.32%\n",
      "Epoch 18  batch 1 . Training Loss:  0.059464868158102036\n",
      "Epoch 18  batch 101 . Training Loss:  0.06839319318532944\n",
      "Epoch 18  batch 201 . Training Loss:  0.08596130460500717\n",
      "Epoch 18  batch 301 . Training Loss:  0.19188225269317627\n",
      "Epoch 18  batch 401 . Training Loss:  0.22456848621368408\n",
      "Epoch 18: Validation Loss: 0.0409, Validation Accuracy: 98.78%\n",
      "Epoch 19  batch 1 . Training Loss:  0.08390334993600845\n",
      "Epoch 19  batch 101 . Training Loss:  0.25549212098121643\n",
      "Epoch 19  batch 201 . Training Loss:  0.07762092351913452\n",
      "Epoch 19  batch 301 . Training Loss:  0.030621472746133804\n",
      "Epoch 19  batch 401 . Training Loss:  0.028770092874765396\n",
      "Epoch 19: Validation Loss: 0.0465, Validation Accuracy: 98.61%\n",
      "Epoch 20  batch 1 . Training Loss:  0.2322929948568344\n",
      "Epoch 20  batch 101 . Training Loss:  0.06059540435671806\n",
      "Epoch 20  batch 201 . Training Loss:  0.058429475873708725\n",
      "Epoch 20  batch 301 . Training Loss:  0.07473070174455643\n",
      "Epoch 20  batch 401 . Training Loss:  0.10744068026542664\n",
      "Epoch 20: Validation Loss: 0.0342, Validation Accuracy: 98.89%\n",
      "Epoch 21  batch 1 . Training Loss:  0.10344704240560532\n",
      "Epoch 21  batch 101 . Training Loss:  0.04241245612502098\n",
      "Epoch 21  batch 201 . Training Loss:  0.09457378834486008\n",
      "Epoch 21  batch 301 . Training Loss:  0.20658092200756073\n",
      "Epoch 21  batch 401 . Training Loss:  0.09214413911104202\n",
      "Epoch 21: Validation Loss: 0.0373, Validation Accuracy: 98.99%\n",
      "Epoch 22  batch 1 . Training Loss:  0.11008656024932861\n",
      "Epoch 22  batch 101 . Training Loss:  0.06462739408016205\n",
      "Epoch 22  batch 201 . Training Loss:  0.11114273220300674\n",
      "Epoch 22  batch 301 . Training Loss:  0.030748702585697174\n",
      "Epoch 22  batch 401 . Training Loss:  0.048293180763721466\n",
      "Epoch 22: Validation Loss: 0.0391, Validation Accuracy: 98.79%\n",
      "Epoch 23  batch 1 . Training Loss:  0.09778373688459396\n",
      "Epoch 23  batch 101 . Training Loss:  0.07673856616020203\n",
      "Epoch 23  batch 201 . Training Loss:  0.060341157019138336\n",
      "Epoch 23  batch 301 . Training Loss:  0.2763488292694092\n",
      "Epoch 23  batch 401 . Training Loss:  0.08108534663915634\n",
      "Epoch 23: Validation Loss: 0.0360, Validation Accuracy: 98.99%\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264e878c2785482d8682ae0d9f4d7c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▅▄▄▃▃▃▂▂▁▂▂▂▁▂▁▁▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.08109</td></tr><tr><td>train_loss</td><td>0.00345</td></tr><tr><td>val_accuracy</td><td>98.99248</td></tr><tr><td>val_loss</td><td>0.03599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_v1</strong> at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/ezmhd20p' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/ezmhd20p</a><br/> View project at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_135058-ezmhd20p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = net\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %100 == 0:\n",
    "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
    "          if enable_wandb:\n",
    "            wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validation_loss = get_loss(val_loader, model, criterion, device)\n",
    "    validation_accuracy = get_accuracy(val_loader, model, device)\n",
    "    wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "    if early_stopping.early_stop(validation_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYQmq5FORqbI"
   },
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21rOVvnYRqV5"
   },
   "source": [
    "### CNN Model - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epochs = 25\n",
    "- Batch size = 128\n",
    "- Learning Rates = 0.001\n",
    "- No Data Agumentation\n",
    "- No Class Balancing \n",
    "- Early Stopping Patience 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mfTZpdJvRqTC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2b50b1fd1e4095aa5ec55a6ba46a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112740722536627, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/evafan/wandb/run-20240417_143325-w1j731qx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/w1j731qx' target=\"_blank\">experiment_v2</a></strong> to <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/w1j731qx' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/w1j731qx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/evafan123/Project%20CNN_v1/runs/w1j731qx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x150cbc34cee0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Project CNN_v1\", name = 'experiment_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uIMFwutlRqPs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 128\n",
    "})\n",
    "wandb.watch(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset_test, shuffle=True, batch_size = 128, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Dropout2d(p=0.4, inplace=False)\n",
       "    (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=256, out_features=43, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = Net(num_classes=43)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net1.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopper(patience=5,delta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nZ3pCZQ7RqM9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.854710817337036\n",
      "Epoch 1  batch 101 . Training Loss:  2.9450109004974365\n",
      "Epoch 1  batch 201 . Training Loss:  2.075486183166504\n",
      "Epoch 1: Validation Loss: 1.8634, Validation Accuracy: 37.39%\n",
      "Epoch 2  batch 1 . Training Loss:  2.2888221740722656\n",
      "Epoch 2  batch 101 . Training Loss:  1.9419071674346924\n",
      "Epoch 2  batch 201 . Training Loss:  1.423435926437378\n",
      "Epoch 2: Validation Loss: 1.1884, Validation Accuracy: 57.85%\n",
      "Epoch 3  batch 1 . Training Loss:  1.5208704471588135\n",
      "Epoch 3  batch 101 . Training Loss:  1.1274008750915527\n",
      "Epoch 3  batch 201 . Training Loss:  0.9119271039962769\n",
      "Epoch 3: Validation Loss: 0.6026, Validation Accuracy: 78.66%\n",
      "Epoch 4  batch 1 . Training Loss:  1.120024561882019\n",
      "Epoch 4  batch 101 . Training Loss:  0.7450882196426392\n",
      "Epoch 4  batch 201 . Training Loss:  0.6549537777900696\n",
      "Epoch 4: Validation Loss: 0.3837, Validation Accuracy: 87.20%\n",
      "Epoch 5  batch 1 . Training Loss:  0.695164680480957\n",
      "Epoch 5  batch 101 . Training Loss:  0.5403280854225159\n",
      "Epoch 5  batch 201 . Training Loss:  0.5035582184791565\n",
      "Epoch 5: Validation Loss: 0.2520, Validation Accuracy: 91.61%\n",
      "Epoch 6  batch 1 . Training Loss:  0.39829206466674805\n",
      "Epoch 6  batch 101 . Training Loss:  0.3111746311187744\n",
      "Epoch 6  batch 201 . Training Loss:  0.3663254380226135\n",
      "Epoch 6: Validation Loss: 0.1758, Validation Accuracy: 94.44%\n",
      "Epoch 7  batch 1 . Training Loss:  0.39829909801483154\n",
      "Epoch 7  batch 101 . Training Loss:  0.35948920249938965\n",
      "Epoch 7  batch 201 . Training Loss:  0.3726517856121063\n",
      "Epoch 7: Validation Loss: 0.1272, Validation Accuracy: 95.93%\n",
      "Epoch 8  batch 1 . Training Loss:  0.21722356975078583\n",
      "Epoch 8  batch 101 . Training Loss:  0.21453669667243958\n",
      "Epoch 8  batch 201 . Training Loss:  0.2269514799118042\n",
      "Epoch 8: Validation Loss: 0.1275, Validation Accuracy: 95.88%\n",
      "Epoch 9  batch 1 . Training Loss:  0.3118630051612854\n",
      "Epoch 9  batch 101 . Training Loss:  0.19931529462337494\n",
      "Epoch 9  batch 201 . Training Loss:  0.15285523235797882\n",
      "Epoch 9: Validation Loss: 0.1199, Validation Accuracy: 95.69%\n",
      "Epoch 10  batch 1 . Training Loss:  0.2398720681667328\n",
      "Epoch 10  batch 101 . Training Loss:  0.16679804027080536\n",
      "Epoch 10  batch 201 . Training Loss:  0.1036018580198288\n",
      "Epoch 10: Validation Loss: 0.0992, Validation Accuracy: 96.81%\n",
      "Epoch 11  batch 1 . Training Loss:  0.11022711545228958\n",
      "Epoch 11  batch 101 . Training Loss:  0.2528921067714691\n",
      "Epoch 11  batch 201 . Training Loss:  0.18484868109226227\n",
      "Epoch 11: Validation Loss: 0.0785, Validation Accuracy: 97.40%\n",
      "Epoch 12  batch 1 . Training Loss:  0.09162359684705734\n",
      "Epoch 12  batch 101 . Training Loss:  0.1508580595254898\n",
      "Epoch 12  batch 201 . Training Loss:  0.12543313205242157\n",
      "Epoch 12: Validation Loss: 0.0779, Validation Accuracy: 97.56%\n",
      "Epoch 13  batch 1 . Training Loss:  0.3044300079345703\n",
      "Epoch 13  batch 101 . Training Loss:  0.17734867334365845\n",
      "Epoch 13  batch 201 . Training Loss:  0.1696525663137436\n",
      "Epoch 13: Validation Loss: 0.0630, Validation Accuracy: 98.09%\n",
      "Epoch 14  batch 1 . Training Loss:  0.08278990536928177\n",
      "Epoch 14  batch 101 . Training Loss:  0.16736362874507904\n",
      "Epoch 14  batch 201 . Training Loss:  0.15163137018680573\n",
      "Epoch 14: Validation Loss: 0.0592, Validation Accuracy: 98.19%\n",
      "Epoch 15  batch 1 . Training Loss:  0.2204219549894333\n",
      "Epoch 15  batch 101 . Training Loss:  0.1881529837846756\n",
      "Epoch 15  batch 201 . Training Loss:  0.18359899520874023\n",
      "Epoch 15: Validation Loss: 0.0565, Validation Accuracy: 98.27%\n",
      "Epoch 16  batch 1 . Training Loss:  0.10340706259012222\n",
      "Epoch 16  batch 101 . Training Loss:  0.1278529018163681\n",
      "Epoch 16  batch 201 . Training Loss:  0.2182907909154892\n",
      "Epoch 16: Validation Loss: 0.0513, Validation Accuracy: 98.57%\n",
      "Epoch 17  batch 1 . Training Loss:  0.11825232952833176\n",
      "Epoch 17  batch 101 . Training Loss:  0.1140456572175026\n",
      "Epoch 17  batch 201 . Training Loss:  0.14956115186214447\n",
      "Epoch 17: Validation Loss: 0.0421, Validation Accuracy: 98.67%\n",
      "Epoch 18  batch 1 . Training Loss:  0.10402627289295197\n",
      "Epoch 18  batch 101 . Training Loss:  0.15357759594917297\n",
      "Epoch 18  batch 201 . Training Loss:  0.11694458872079849\n",
      "Epoch 18: Validation Loss: 0.0469, Validation Accuracy: 98.48%\n",
      "Epoch 19  batch 1 . Training Loss:  0.08959630131721497\n",
      "Epoch 19  batch 101 . Training Loss:  0.15861979126930237\n",
      "Epoch 19  batch 201 . Training Loss:  0.06282872706651688\n",
      "Epoch 19: Validation Loss: 0.0379, Validation Accuracy: 98.98%\n",
      "Epoch 20  batch 1 . Training Loss:  0.04823574423789978\n",
      "Epoch 20  batch 101 . Training Loss:  0.10689842700958252\n",
      "Epoch 20  batch 201 . Training Loss:  0.11903122067451477\n",
      "Epoch 20: Validation Loss: 0.0433, Validation Accuracy: 98.65%\n",
      "Epoch 21  batch 1 . Training Loss:  0.07161493599414825\n",
      "Epoch 21  batch 101 . Training Loss:  0.1330019235610962\n",
      "Epoch 21  batch 201 . Training Loss:  0.22916877269744873\n",
      "Epoch 21: Validation Loss: 0.0367, Validation Accuracy: 98.94%\n",
      "Epoch 22  batch 1 . Training Loss:  0.09565196931362152\n",
      "Epoch 22  batch 101 . Training Loss:  0.09482132643461227\n",
      "Epoch 22  batch 201 . Training Loss:  0.05205780267715454\n",
      "Epoch 22: Validation Loss: 0.0378, Validation Accuracy: 98.72%\n",
      "Epoch 23  batch 1 . Training Loss:  0.06125681474804878\n",
      "Epoch 23  batch 101 . Training Loss:  0.046858109533786774\n",
      "Epoch 23  batch 201 . Training Loss:  0.08512814342975616\n",
      "Epoch 23: Validation Loss: 0.0353, Validation Accuracy: 99.12%\n",
      "Epoch 24  batch 1 . Training Loss:  0.08163134008646011\n",
      "Epoch 24  batch 101 . Training Loss:  0.10088887810707092\n",
      "Epoch 24  batch 201 . Training Loss:  0.045532889664173126\n",
      "Epoch 24: Validation Loss: 0.0359, Validation Accuracy: 99.07%\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad995eac958547e88dfc2fad197b3591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▅▄▃▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▆▇▇▇██████████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.04553</td></tr><tr><td>train_loss</td><td>0.00129</td></tr><tr><td>val_accuracy</td><td>99.069</td></tr><tr><td>val_loss</td><td>0.03595</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_v2</strong> at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/w1j731qx' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/w1j731qx</a><br/> View project at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_143325-w1j731qx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 25\n",
    "model = net1\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %100 == 0:\n",
    "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
    "          if enable_wandb:\n",
    "            wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validation_loss = get_loss(val_loader, model, criterion, device)\n",
    "    validation_accuracy = get_accuracy(val_loader, model, device)\n",
    "    wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "    if early_stopping.early_stop(validation_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHBREy3FRqJx"
   },
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epochs = 25\n",
    "- Batch size = 128\n",
    "- Learning Rates = 0.001\n",
    "- No Data Agumentation\n",
    "- Class Balancing \n",
    "- Early Stopping Patience 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/evafan/wandb/run-20240417_160729-wtq186fx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/wtq186fx' target=\"_blank\">experiment_v3</a></strong> to <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/wtq186fx' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/wtq186fx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/evafan123/Project%20CNN_v1/runs/wtq186fx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x150c25ad2fe0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Project CNN_v1\", name = 'experiment_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 128\n",
    "})\n",
    "wandb.watch(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset_test, shuffle=True, batch_size = 128, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Dropout2d(p=0.4, inplace=False)\n",
       "    (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=256, out_features=43, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = Net(num_classes=43)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [total_train/sorted_train[str(i).zfill(2)] if str(i).zfill(2) in sorted_train else 1 for i in range(43)]\n",
    "class_weights = torch.FloatTensor(weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net2.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopper(patience=5,delta = 0.01)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.7736706733703613\n",
      "Epoch 1  batch 101 . Training Loss:  3.5578105449676514\n",
      "Epoch 1  batch 201 . Training Loss:  3.195345163345337\n",
      "Epoch 1: Validation Loss: 3.0196, Validation Accuracy: 5.17%\n",
      "Epoch 2  batch 1 . Training Loss:  2.9793307781219482\n",
      "Epoch 2  batch 101 . Training Loss:  2.6133809089660645\n",
      "Epoch 2  batch 201 . Training Loss:  2.442598342895508\n",
      "Epoch 2: Validation Loss: 2.0729, Validation Accuracy: 29.78%\n",
      "Epoch 3  batch 1 . Training Loss:  2.336299419403076\n",
      "Epoch 3  batch 101 . Training Loss:  2.1039879322052\n",
      "Epoch 3  batch 201 . Training Loss:  2.08070969581604\n",
      "Epoch 3: Validation Loss: 1.6997, Validation Accuracy: 39.69%\n",
      "Epoch 4  batch 1 . Training Loss:  1.8897264003753662\n",
      "Epoch 4  batch 101 . Training Loss:  2.063704490661621\n",
      "Epoch 4  batch 201 . Training Loss:  1.7049708366394043\n",
      "Epoch 4: Validation Loss: 1.2603, Validation Accuracy: 50.34%\n",
      "Epoch 5  batch 1 . Training Loss:  1.4017705917358398\n",
      "Epoch 5  batch 101 . Training Loss:  1.4644529819488525\n",
      "Epoch 5  batch 201 . Training Loss:  1.1759403944015503\n",
      "Epoch 5: Validation Loss: 0.9108, Validation Accuracy: 63.46%\n",
      "Epoch 6  batch 1 . Training Loss:  1.316703200340271\n",
      "Epoch 6  batch 101 . Training Loss:  1.1272741556167603\n",
      "Epoch 6  batch 201 . Training Loss:  1.1494197845458984\n",
      "Epoch 6: Validation Loss: 0.5792, Validation Accuracy: 72.55%\n",
      "Epoch 7  batch 1 . Training Loss:  0.9424355626106262\n",
      "Epoch 7  batch 101 . Training Loss:  0.9627170562744141\n",
      "Epoch 7  batch 201 . Training Loss:  0.7132917046546936\n",
      "Epoch 7: Validation Loss: 0.4527, Validation Accuracy: 76.48%\n",
      "Epoch 8  batch 1 . Training Loss:  0.739297091960907\n",
      "Epoch 8  batch 101 . Training Loss:  0.5749643445014954\n",
      "Epoch 8  batch 201 . Training Loss:  0.5105191469192505\n",
      "Epoch 8: Validation Loss: 0.3419, Validation Accuracy: 81.88%\n",
      "Epoch 9  batch 1 . Training Loss:  0.7816896438598633\n",
      "Epoch 9  batch 101 . Training Loss:  0.4045046865940094\n",
      "Epoch 9  batch 201 . Training Loss:  0.3747624158859253\n",
      "Epoch 9: Validation Loss: 0.2859, Validation Accuracy: 85.96%\n",
      "Epoch 10  batch 1 . Training Loss:  0.718317985534668\n",
      "Epoch 10  batch 101 . Training Loss:  0.4254109859466553\n",
      "Epoch 10  batch 201 . Training Loss:  0.5554942488670349\n",
      "Epoch 10: Validation Loss: 0.2366, Validation Accuracy: 87.77%\n",
      "Epoch 11  batch 1 . Training Loss:  0.4264855682849884\n",
      "Epoch 11  batch 101 . Training Loss:  0.44754305481910706\n",
      "Epoch 11  batch 201 . Training Loss:  0.6361220479011536\n",
      "Epoch 11: Validation Loss: 0.2023, Validation Accuracy: 89.71%\n",
      "Epoch 12  batch 1 . Training Loss:  0.3716249465942383\n",
      "Epoch 12  batch 101 . Training Loss:  0.30156829953193665\n",
      "Epoch 12  batch 201 . Training Loss:  0.24838313460350037\n",
      "Epoch 12: Validation Loss: 0.1727, Validation Accuracy: 91.17%\n",
      "Epoch 13  batch 1 . Training Loss:  0.37051329016685486\n",
      "Epoch 13  batch 101 . Training Loss:  0.3623619079589844\n",
      "Epoch 13  batch 201 . Training Loss:  0.26972848176956177\n",
      "Epoch 13: Validation Loss: 0.1520, Validation Accuracy: 91.83%\n",
      "Epoch 14  batch 1 . Training Loss:  0.2665461003780365\n",
      "Epoch 14  batch 101 . Training Loss:  0.3070172071456909\n",
      "Epoch 14  batch 201 . Training Loss:  0.4754333198070526\n",
      "Epoch 14: Validation Loss: 0.1451, Validation Accuracy: 93.09%\n",
      "Epoch 15  batch 1 . Training Loss:  0.430475115776062\n",
      "Epoch 15  batch 101 . Training Loss:  0.2857552170753479\n",
      "Epoch 15  batch 201 . Training Loss:  0.1850673109292984\n",
      "Epoch 15: Validation Loss: 0.1222, Validation Accuracy: 94.12%\n",
      "Epoch 16  batch 1 . Training Loss:  0.28315526247024536\n",
      "Epoch 16  batch 101 . Training Loss:  0.2956596612930298\n",
      "Epoch 16  batch 201 . Training Loss:  0.2788715362548828\n",
      "Epoch 16: Validation Loss: 0.1230, Validation Accuracy: 93.97%\n",
      "Epoch 17  batch 1 . Training Loss:  0.3437974750995636\n",
      "Epoch 17  batch 101 . Training Loss:  0.2953653335571289\n",
      "Epoch 17  batch 201 . Training Loss:  0.3020947277545929\n",
      "Epoch 17: Validation Loss: 0.1011, Validation Accuracy: 95.32%\n",
      "Epoch 18  batch 1 . Training Loss:  0.224470317363739\n",
      "Epoch 18  batch 101 . Training Loss:  0.24974258244037628\n",
      "Epoch 18  batch 201 . Training Loss:  0.2252579778432846\n",
      "Epoch 18: Validation Loss: 0.0900, Validation Accuracy: 95.70%\n",
      "Epoch 19  batch 1 . Training Loss:  0.19967585802078247\n",
      "Epoch 19  batch 101 . Training Loss:  0.29124659299850464\n",
      "Epoch 19  batch 201 . Training Loss:  0.14317406713962555\n",
      "Epoch 19: Validation Loss: 0.0805, Validation Accuracy: 96.33%\n",
      "Epoch 20  batch 1 . Training Loss:  0.16023410856723785\n",
      "Epoch 20  batch 101 . Training Loss:  0.18482881784439087\n",
      "Epoch 20  batch 201 . Training Loss:  0.17997337877750397\n",
      "Epoch 20: Validation Loss: 0.0797, Validation Accuracy: 96.15%\n",
      "Epoch 21  batch 1 . Training Loss:  0.17919614911079407\n",
      "Epoch 21  batch 101 . Training Loss:  0.3174866735935211\n",
      "Epoch 21  batch 201 . Training Loss:  0.17121846973896027\n",
      "Epoch 21: Validation Loss: 0.0667, Validation Accuracy: 96.67%\n",
      "Epoch 22  batch 1 . Training Loss:  0.14969351887702942\n",
      "Epoch 22  batch 101 . Training Loss:  0.18760204315185547\n",
      "Epoch 22  batch 201 . Training Loss:  0.15528038144111633\n",
      "Epoch 22: Validation Loss: 0.0720, Validation Accuracy: 96.94%\n",
      "Epoch 23  batch 1 . Training Loss:  0.3391406834125519\n",
      "Epoch 23  batch 101 . Training Loss:  0.17139706015586853\n",
      "Epoch 23  batch 201 . Training Loss:  0.12342008948326111\n",
      "Epoch 23: Validation Loss: 0.0637, Validation Accuracy: 97.02%\n",
      "Epoch 24  batch 1 . Training Loss:  0.259400874376297\n",
      "Epoch 24  batch 101 . Training Loss:  0.1115034818649292\n",
      "Epoch 24  batch 201 . Training Loss:  0.0891747921705246\n",
      "Epoch 24: Validation Loss: 0.0674, Validation Accuracy: 97.00%\n",
      "Epoch 25  batch 1 . Training Loss:  0.15927934646606445\n",
      "Epoch 25  batch 101 . Training Loss:  0.14029788970947266\n",
      "Epoch 25  batch 201 . Training Loss:  0.1845475435256958\n",
      "Epoch 25: Validation Loss: 0.0578, Validation Accuracy: 97.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407e1db1a0c448f4846b3943098d6a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>██▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▆▆▇▇▇▇██████████████</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.18455</td></tr><tr><td>train_loss</td><td>0.01202</td></tr><tr><td>val_accuracy</td><td>97.65336</td></tr><tr><td>val_loss</td><td>0.05779</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_v3</strong> at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/wtq186fx' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/wtq186fx</a><br/> View project at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_160729-wtq186fx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 25\n",
    "model = net2\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %100 == 0:\n",
    "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
    "          if enable_wandb:\n",
    "            wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validation_loss = get_loss(val_loader, model, criterion, device)\n",
    "    validation_accuracy = get_accuracy(val_loader, model, device)\n",
    "    wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "    if early_stopping.early_stop(validation_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epochs = 50\n",
    "- Batch size = 128\n",
    "- Learning Rates = 0.001\n",
    "- Data Agumentation\n",
    "- Class Balancing \n",
    "- Early Stopping Patience 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/evafan/wandb/run-20240417_163104-vl1uu89m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vl1uu89m' target=\"_blank\">experiment_v4</a></strong> to <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vl1uu89m' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/vl1uu89m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vl1uu89m?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x150c25a71d20>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Project CNN_v1\", name = 'experiment_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_1 = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), \n",
    "    transforms.RandomHorizontalFlip(0.5), \n",
    "    transforms.RandomRotation(45), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), (0.2672, 0.2564, 0.2629))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ImageFolder(\n",
    "    'Train',\n",
    "    transform = train_transform_1\n",
    ")\n",
    "\n",
    "train_data, val_data = random_split(dataset_train, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset_test, shuffle=True, batch_size = 128, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Dropout2d(p=0.4, inplace=False)\n",
       "    (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=256, out_features=43, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3 = Net(num_classes=43)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net3.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopper(patience=5,delta = 0.01)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.8147876262664795\n",
      "Epoch 1  batch 101 . Training Loss:  3.5149896144866943\n",
      "Epoch 1  batch 201 . Training Loss:  3.3080899715423584\n",
      "Epoch 1: Validation Loss: 2.8435, Validation Accuracy: 8.86%\n",
      "Epoch 2  batch 1 . Training Loss:  2.7447104454040527\n",
      "Epoch 2  batch 101 . Training Loss:  2.8544790744781494\n",
      "Epoch 2  batch 201 . Training Loss:  2.420358657836914\n",
      "Epoch 2: Validation Loss: 2.2796, Validation Accuracy: 21.16%\n",
      "Epoch 3  batch 1 . Training Loss:  2.4717936515808105\n",
      "Epoch 3  batch 101 . Training Loss:  2.3563475608825684\n",
      "Epoch 3  batch 201 . Training Loss:  2.222247362136841\n",
      "Epoch 3: Validation Loss: 2.0543, Validation Accuracy: 25.69%\n",
      "Epoch 4  batch 1 . Training Loss:  2.1091411113739014\n",
      "Epoch 4  batch 101 . Training Loss:  2.2933366298675537\n",
      "Epoch 4  batch 201 . Training Loss:  2.1197264194488525\n",
      "Epoch 4: Validation Loss: 1.8142, Validation Accuracy: 34.61%\n",
      "Epoch 5  batch 1 . Training Loss:  1.878069519996643\n",
      "Epoch 5  batch 101 . Training Loss:  1.8093887567520142\n",
      "Epoch 5  batch 201 . Training Loss:  1.7186288833618164\n",
      "Epoch 5: Validation Loss: 1.6591, Validation Accuracy: 39.80%\n",
      "Epoch 6  batch 1 . Training Loss:  1.6346933841705322\n",
      "Epoch 6  batch 101 . Training Loss:  2.0103840827941895\n",
      "Epoch 6  batch 201 . Training Loss:  1.734400749206543\n",
      "Epoch 6: Validation Loss: 1.4223, Validation Accuracy: 41.64%\n",
      "Epoch 7  batch 1 . Training Loss:  1.53714919090271\n",
      "Epoch 7  batch 101 . Training Loss:  1.7284539937973022\n",
      "Epoch 7  batch 201 . Training Loss:  1.4583876132965088\n",
      "Epoch 7: Validation Loss: 1.3287, Validation Accuracy: 45.58%\n",
      "Epoch 8  batch 1 . Training Loss:  1.8070791959762573\n",
      "Epoch 8  batch 101 . Training Loss:  1.537049651145935\n",
      "Epoch 8  batch 201 . Training Loss:  1.626383662223816\n",
      "Epoch 8: Validation Loss: 1.1853, Validation Accuracy: 50.70%\n",
      "Epoch 9  batch 1 . Training Loss:  1.350841760635376\n",
      "Epoch 9  batch 101 . Training Loss:  1.1847546100616455\n",
      "Epoch 9  batch 201 . Training Loss:  1.3897762298583984\n",
      "Epoch 9: Validation Loss: 1.0657, Validation Accuracy: 55.12%\n",
      "Epoch 10  batch 1 . Training Loss:  1.2967913150787354\n",
      "Epoch 10  batch 101 . Training Loss:  1.3542358875274658\n",
      "Epoch 10  batch 201 . Training Loss:  1.3694632053375244\n",
      "Epoch 10: Validation Loss: 0.9649, Validation Accuracy: 57.39%\n",
      "Epoch 11  batch 1 . Training Loss:  1.5101908445358276\n",
      "Epoch 11  batch 101 . Training Loss:  1.389530897140503\n",
      "Epoch 11  batch 201 . Training Loss:  1.3294150829315186\n",
      "Epoch 11: Validation Loss: 0.9056, Validation Accuracy: 59.20%\n",
      "Epoch 12  batch 1 . Training Loss:  1.2073345184326172\n",
      "Epoch 12  batch 101 . Training Loss:  1.2742961645126343\n",
      "Epoch 12  batch 201 . Training Loss:  0.9137673377990723\n",
      "Epoch 12: Validation Loss: 0.8638, Validation Accuracy: 60.96%\n",
      "Epoch 13  batch 1 . Training Loss:  1.2139540910720825\n",
      "Epoch 13  batch 101 . Training Loss:  1.0828068256378174\n",
      "Epoch 13  batch 201 . Training Loss:  1.146146297454834\n",
      "Epoch 13: Validation Loss: 0.8044, Validation Accuracy: 62.10%\n",
      "Epoch 14  batch 1 . Training Loss:  0.9359943866729736\n",
      "Epoch 14  batch 101 . Training Loss:  1.0674169063568115\n",
      "Epoch 14  batch 201 . Training Loss:  1.1619030237197876\n",
      "Epoch 14: Validation Loss: 0.7590, Validation Accuracy: 64.41%\n",
      "Epoch 15  batch 1 . Training Loss:  1.0161744356155396\n",
      "Epoch 15  batch 101 . Training Loss:  1.0245864391326904\n",
      "Epoch 15  batch 201 . Training Loss:  0.840387761592865\n",
      "Epoch 15: Validation Loss: 0.7301, Validation Accuracy: 66.52%\n",
      "Epoch 16  batch 1 . Training Loss:  0.9238609671592712\n",
      "Epoch 16  batch 101 . Training Loss:  0.8337833285331726\n",
      "Epoch 16  batch 201 . Training Loss:  1.012507677078247\n",
      "Epoch 16: Validation Loss: 0.7127, Validation Accuracy: 66.88%\n",
      "Epoch 17  batch 1 . Training Loss:  0.8588206768035889\n",
      "Epoch 17  batch 101 . Training Loss:  0.9264667630195618\n",
      "Epoch 17  batch 201 . Training Loss:  1.130476474761963\n",
      "Epoch 17: Validation Loss: 0.6710, Validation Accuracy: 71.13%\n",
      "Epoch 18  batch 1 . Training Loss:  0.8537394404411316\n",
      "Epoch 18  batch 101 . Training Loss:  0.7621443271636963\n",
      "Epoch 18  batch 201 . Training Loss:  1.026057481765747\n",
      "Epoch 18: Validation Loss: 0.6141, Validation Accuracy: 72.41%\n",
      "Epoch 19  batch 1 . Training Loss:  0.851945698261261\n",
      "Epoch 19  batch 101 . Training Loss:  1.0049759149551392\n",
      "Epoch 19  batch 201 . Training Loss:  0.8236258029937744\n",
      "Epoch 19: Validation Loss: 0.5859, Validation Accuracy: 71.00%\n",
      "Epoch 20  batch 1 . Training Loss:  0.8219751715660095\n",
      "Epoch 20  batch 101 . Training Loss:  0.8330689072608948\n",
      "Epoch 20  batch 201 . Training Loss:  0.8547568917274475\n",
      "Epoch 20: Validation Loss: 0.5701, Validation Accuracy: 73.46%\n",
      "Epoch 21  batch 1 . Training Loss:  0.7234737873077393\n",
      "Epoch 21  batch 101 . Training Loss:  0.9061400890350342\n",
      "Epoch 21  batch 201 . Training Loss:  0.8175577521324158\n",
      "Epoch 21: Validation Loss: 0.5405, Validation Accuracy: 73.94%\n",
      "Epoch 22  batch 1 . Training Loss:  0.7432879209518433\n",
      "Epoch 22  batch 101 . Training Loss:  0.8523425459861755\n",
      "Epoch 22  batch 201 . Training Loss:  0.9113644361495972\n",
      "Epoch 22: Validation Loss: 0.5439, Validation Accuracy: 73.77%\n",
      "Epoch 23  batch 1 . Training Loss:  0.616101086139679\n",
      "Epoch 23  batch 101 . Training Loss:  0.747808039188385\n",
      "Epoch 23  batch 201 . Training Loss:  0.6548805832862854\n",
      "Epoch 23: Validation Loss: 0.4760, Validation Accuracy: 77.18%\n",
      "Epoch 24  batch 1 . Training Loss:  0.5703667998313904\n",
      "Epoch 24  batch 101 . Training Loss:  0.7740985155105591\n",
      "Epoch 24  batch 201 . Training Loss:  0.7180878520011902\n",
      "Epoch 24: Validation Loss: 0.4764, Validation Accuracy: 77.66%\n",
      "Epoch 25  batch 1 . Training Loss:  0.7559589147567749\n",
      "Epoch 25  batch 101 . Training Loss:  0.6589093208312988\n",
      "Epoch 25  batch 201 . Training Loss:  0.829163134098053\n",
      "Epoch 25: Validation Loss: 0.4616, Validation Accuracy: 77.40%\n",
      "Epoch 26  batch 1 . Training Loss:  0.6960063576698303\n",
      "Epoch 26  batch 101 . Training Loss:  0.6012942790985107\n",
      "Epoch 26  batch 201 . Training Loss:  0.6219682097434998\n",
      "Epoch 26: Validation Loss: 0.4366, Validation Accuracy: 78.14%\n",
      "Epoch 27  batch 1 . Training Loss:  0.6142142415046692\n",
      "Epoch 27  batch 101 . Training Loss:  0.6537884473800659\n",
      "Epoch 27  batch 201 . Training Loss:  0.7637179493904114\n",
      "Epoch 27: Validation Loss: 0.4147, Validation Accuracy: 77.41%\n",
      "Epoch 28  batch 1 . Training Loss:  0.7085140943527222\n",
      "Epoch 28  batch 101 . Training Loss:  0.7162855267524719\n",
      "Epoch 28  batch 201 . Training Loss:  0.711592435836792\n",
      "Epoch 28: Validation Loss: 0.4214, Validation Accuracy: 79.72%\n",
      "Epoch 29  batch 1 . Training Loss:  0.635911226272583\n",
      "Epoch 29  batch 101 . Training Loss:  0.7518412470817566\n",
      "Epoch 29  batch 201 . Training Loss:  0.6384578943252563\n",
      "Epoch 29: Validation Loss: 0.3989, Validation Accuracy: 79.96%\n",
      "Epoch 30  batch 1 . Training Loss:  0.694694995880127\n",
      "Epoch 30  batch 101 . Training Loss:  0.5512910485267639\n",
      "Epoch 30  batch 201 . Training Loss:  0.8027809858322144\n",
      "Epoch 30: Validation Loss: 0.4001, Validation Accuracy: 78.59%\n",
      "Epoch 31  batch 1 . Training Loss:  0.5098328590393066\n",
      "Epoch 31  batch 101 . Training Loss:  0.5668683648109436\n",
      "Epoch 31  batch 201 . Training Loss:  0.6177315711975098\n",
      "Epoch 31: Validation Loss: 0.4554, Validation Accuracy: 77.59%\n",
      "Epoch 32  batch 1 . Training Loss:  0.7404090762138367\n",
      "Epoch 32  batch 101 . Training Loss:  0.49081236124038696\n",
      "Epoch 32  batch 201 . Training Loss:  0.7317051887512207\n",
      "Epoch 32: Validation Loss: 0.3477, Validation Accuracy: 81.00%\n",
      "Epoch 33  batch 1 . Training Loss:  0.6059399247169495\n",
      "Epoch 33  batch 101 . Training Loss:  0.4585944414138794\n",
      "Epoch 33  batch 201 . Training Loss:  0.6452369689941406\n",
      "Epoch 33: Validation Loss: 0.3455, Validation Accuracy: 81.95%\n",
      "Epoch 34  batch 1 . Training Loss:  0.6165857315063477\n",
      "Epoch 34  batch 101 . Training Loss:  0.7073436379432678\n",
      "Epoch 34  batch 201 . Training Loss:  0.7555443644523621\n",
      "Epoch 34: Validation Loss: 0.3434, Validation Accuracy: 82.06%\n",
      "Epoch 35  batch 1 . Training Loss:  0.5331928730010986\n",
      "Epoch 35  batch 101 . Training Loss:  0.5801885724067688\n",
      "Epoch 35  batch 201 . Training Loss:  0.7208600640296936\n",
      "Epoch 35: Validation Loss: 0.3173, Validation Accuracy: 83.47%\n",
      "Epoch 36  batch 1 . Training Loss:  0.49050045013427734\n",
      "Epoch 36  batch 101 . Training Loss:  0.5660935044288635\n",
      "Epoch 36  batch 201 . Training Loss:  0.5312013030052185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Validation Loss: 0.3294, Validation Accuracy: 82.72%\n",
      "Epoch 37  batch 1 . Training Loss:  0.45854634046554565\n",
      "Epoch 37  batch 101 . Training Loss:  0.42244046926498413\n",
      "Epoch 37  batch 201 . Training Loss:  0.6222683191299438\n",
      "Epoch 37: Validation Loss: 0.3092, Validation Accuracy: 84.29%\n",
      "Epoch 38  batch 1 . Training Loss:  0.3853470981121063\n",
      "Epoch 38  batch 101 . Training Loss:  0.5691140294075012\n",
      "Epoch 38  batch 201 . Training Loss:  0.4466889798641205\n",
      "Epoch 38: Validation Loss: 0.3283, Validation Accuracy: 82.64%\n",
      "Epoch 39  batch 1 . Training Loss:  0.4012397229671478\n",
      "Epoch 39  batch 101 . Training Loss:  0.46965929865837097\n",
      "Epoch 39  batch 201 . Training Loss:  0.46745169162750244\n",
      "Epoch 39: Validation Loss: 0.3072, Validation Accuracy: 83.88%\n",
      "Epoch 40  batch 1 . Training Loss:  0.6425666809082031\n",
      "Epoch 40  batch 101 . Training Loss:  0.566315233707428\n",
      "Epoch 40  batch 201 . Training Loss:  0.466322660446167\n",
      "Epoch 40: Validation Loss: 0.2996, Validation Accuracy: 83.76%\n",
      "Epoch 41  batch 1 . Training Loss:  0.5310930609703064\n",
      "Epoch 41  batch 101 . Training Loss:  0.5215045213699341\n",
      "Epoch 41  batch 201 . Training Loss:  0.5798084735870361\n",
      "Epoch 41: Validation Loss: 0.2999, Validation Accuracy: 84.84%\n",
      "Epoch 42  batch 1 . Training Loss:  0.5583769083023071\n",
      "Epoch 42  batch 101 . Training Loss:  0.6247722506523132\n",
      "Epoch 42  batch 201 . Training Loss:  0.5147913098335266\n",
      "Epoch 42: Validation Loss: 0.2703, Validation Accuracy: 84.76%\n",
      "Epoch 43  batch 1 . Training Loss:  0.43431639671325684\n",
      "Epoch 43  batch 101 . Training Loss:  0.5692195296287537\n",
      "Epoch 43  batch 201 . Training Loss:  0.3846636712551117\n",
      "Epoch 43: Validation Loss: 0.2750, Validation Accuracy: 84.66%\n",
      "Epoch 44  batch 1 . Training Loss:  0.4412136375904083\n",
      "Epoch 44  batch 101 . Training Loss:  0.3173951506614685\n",
      "Epoch 44  batch 201 . Training Loss:  0.7884201407432556\n",
      "Epoch 44: Validation Loss: 0.2602, Validation Accuracy: 85.13%\n",
      "Epoch 45  batch 1 . Training Loss:  0.5830985307693481\n",
      "Epoch 45  batch 101 . Training Loss:  0.5345719456672668\n",
      "Epoch 45  batch 201 . Training Loss:  0.4846295416355133\n",
      "Epoch 45: Validation Loss: 0.2531, Validation Accuracy: 85.78%\n",
      "Epoch 46  batch 1 . Training Loss:  0.40691670775413513\n",
      "Epoch 46  batch 101 . Training Loss:  0.3299926221370697\n",
      "Epoch 46  batch 201 . Training Loss:  0.4986615777015686\n",
      "Epoch 46: Validation Loss: 0.2710, Validation Accuracy: 85.44%\n",
      "Epoch 47  batch 1 . Training Loss:  0.332743376493454\n",
      "Epoch 47  batch 101 . Training Loss:  0.38155338168144226\n",
      "Epoch 47  batch 201 . Training Loss:  0.447689950466156\n",
      "Epoch 47: Validation Loss: 0.2419, Validation Accuracy: 86.24%\n",
      "Epoch 48  batch 1 . Training Loss:  0.4418121576309204\n",
      "Epoch 48  batch 101 . Training Loss:  0.5130907893180847\n",
      "Epoch 48  batch 201 . Training Loss:  0.5625210404396057\n",
      "Epoch 48: Validation Loss: 0.2673, Validation Accuracy: 84.79%\n",
      "Epoch 49  batch 1 . Training Loss:  0.5995877981185913\n",
      "Epoch 49  batch 101 . Training Loss:  0.43377718329429626\n",
      "Epoch 49  batch 201 . Training Loss:  0.4133082926273346\n",
      "Epoch 49: Validation Loss: 0.2419, Validation Accuracy: 87.86%\n",
      "Epoch 50  batch 1 . Training Loss:  0.39605093002319336\n",
      "Epoch 50  batch 101 . Training Loss:  0.364484041929245\n",
      "Epoch 50  batch 201 . Training Loss:  0.34472334384918213\n",
      "Epoch 50: Validation Loss: 0.2300, Validation Accuracy: 87.41%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bd4c0b25324823819070fc7585b045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▅▅▄▄▄▃▃▂▃▃▂▂▃▂▂▂▁▂▂▂▂▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▄▃▄▃▃▃▄▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▂▁▁▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_loss</td><td>█▆▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.34472</td></tr><tr><td>train_loss</td><td>0.48425</td></tr><tr><td>val_accuracy</td><td>87.41232</td></tr><tr><td>val_loss</td><td>0.23001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_v4</strong> at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vl1uu89m' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/vl1uu89m</a><br/> View project at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_163104-vl1uu89m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 50\n",
    "model = net3\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %100 == 0:\n",
    "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
    "          if enable_wandb:\n",
    "            wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validation_loss = get_loss(val_loader, model, criterion, device)\n",
    "    validation_accuracy = get_accuracy(val_loader, model, device)\n",
    "    wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "    if early_stopping.early_stop(validation_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epochs = 100\n",
    "- Batch size = 128\n",
    "- Learning Rates = 0.001\n",
    "- Data Agumentation\n",
    "- Class Balancing \n",
    "- Early Stopping Patience 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/evafan/wandb/run-20240417_165937-7glnky47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/7glnky47' target=\"_blank\">experiment_v5</a></strong> to <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/7glnky47' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/7glnky47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/evafan123/Project%20CNN_v1/runs/7glnky47?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x150c25ac8970>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Project CNN_v1\", name = 'experiment_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset_test, shuffle=True, batch_size = 128, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Dropout2d(p=0.4, inplace=False)\n",
       "    (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=256, out_features=43, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net4 = Net(num_classes=43)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net4.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_normalized = [weight * len(sorted_train) / sum(weights) for weight in weights]\n",
    "class_weights = torch.tensor(weights_normalized, dtype=torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "    class_weights = class_weights.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net4.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopper(patience=5,delta = 0.01)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.7799925804138184\n",
      "Epoch 1  batch 101 . Training Loss:  3.462458848953247\n",
      "Epoch 1  batch 201 . Training Loss:  2.818197011947632\n",
      "Epoch 1: Validation Loss: 2.9514, Validation Accuracy: 8.23%\n",
      "Epoch 2  batch 1 . Training Loss:  2.760685920715332\n",
      "Epoch 2  batch 101 . Training Loss:  2.5269665718078613\n",
      "Epoch 2  batch 201 . Training Loss:  2.2609143257141113\n",
      "Epoch 2: Validation Loss: 2.2359, Validation Accuracy: 26.31%\n",
      "Epoch 3  batch 1 . Training Loss:  2.2194390296936035\n",
      "Epoch 3  batch 101 . Training Loss:  2.5049893856048584\n",
      "Epoch 3  batch 201 . Training Loss:  2.280463695526123\n",
      "Epoch 3: Validation Loss: 1.9806, Validation Accuracy: 33.39%\n",
      "Epoch 4  batch 1 . Training Loss:  2.4278275966644287\n",
      "Epoch 4  batch 101 . Training Loss:  2.079555034637451\n",
      "Epoch 4  batch 201 . Training Loss:  2.0913970470428467\n",
      "Epoch 4: Validation Loss: 1.8345, Validation Accuracy: 33.64%\n",
      "Epoch 5  batch 1 . Training Loss:  1.8381377458572388\n",
      "Epoch 5  batch 101 . Training Loss:  1.9224226474761963\n",
      "Epoch 5  batch 201 . Training Loss:  1.7102911472320557\n",
      "Epoch 5: Validation Loss: 1.5542, Validation Accuracy: 41.64%\n",
      "Epoch 6  batch 1 . Training Loss:  1.8724088668823242\n",
      "Epoch 6  batch 101 . Training Loss:  1.6865379810333252\n",
      "Epoch 6  batch 201 . Training Loss:  1.5590516328811646\n",
      "Epoch 6: Validation Loss: 1.4087, Validation Accuracy: 44.52%\n",
      "Epoch 7  batch 1 . Training Loss:  1.8031749725341797\n",
      "Epoch 7  batch 101 . Training Loss:  1.5023640394210815\n",
      "Epoch 7  batch 201 . Training Loss:  1.7307835817337036\n",
      "Epoch 7: Validation Loss: 1.2767, Validation Accuracy: 49.59%\n",
      "Epoch 8  batch 1 . Training Loss:  1.6356778144836426\n",
      "Epoch 8  batch 101 . Training Loss:  1.3547987937927246\n",
      "Epoch 8  batch 201 . Training Loss:  1.3978163003921509\n",
      "Epoch 8: Validation Loss: 1.1724, Validation Accuracy: 51.41%\n",
      "Epoch 9  batch 1 . Training Loss:  1.4499832391738892\n",
      "Epoch 9  batch 101 . Training Loss:  1.3337634801864624\n",
      "Epoch 9  batch 201 . Training Loss:  1.3923139572143555\n",
      "Epoch 9: Validation Loss: 1.0434, Validation Accuracy: 55.20%\n",
      "Epoch 10  batch 1 . Training Loss:  1.378868579864502\n",
      "Epoch 10  batch 101 . Training Loss:  1.342711329460144\n",
      "Epoch 10  batch 201 . Training Loss:  1.4420387744903564\n",
      "Epoch 10: Validation Loss: 0.9725, Validation Accuracy: 60.89%\n",
      "Epoch 11  batch 1 . Training Loss:  1.2944505214691162\n",
      "Epoch 11  batch 101 . Training Loss:  1.4743415117263794\n",
      "Epoch 11  batch 201 . Training Loss:  1.3430593013763428\n",
      "Epoch 11: Validation Loss: 0.9120, Validation Accuracy: 59.33%\n",
      "Epoch 12  batch 1 . Training Loss:  1.007018804550171\n",
      "Epoch 12  batch 101 . Training Loss:  1.2766239643096924\n",
      "Epoch 12  batch 201 . Training Loss:  1.3544987440109253\n",
      "Epoch 12: Validation Loss: 0.8717, Validation Accuracy: 62.43%\n",
      "Epoch 13  batch 1 . Training Loss:  1.219872236251831\n",
      "Epoch 13  batch 101 . Training Loss:  1.3780308961868286\n",
      "Epoch 13  batch 201 . Training Loss:  1.0606452226638794\n",
      "Epoch 13: Validation Loss: 0.7767, Validation Accuracy: 67.33%\n",
      "Epoch 14  batch 1 . Training Loss:  1.0794702768325806\n",
      "Epoch 14  batch 101 . Training Loss:  0.9292933344841003\n",
      "Epoch 14  batch 201 . Training Loss:  0.9144702553749084\n",
      "Epoch 14: Validation Loss: 0.7621, Validation Accuracy: 66.41%\n",
      "Epoch 15  batch 1 . Training Loss:  0.9097509980201721\n",
      "Epoch 15  batch 101 . Training Loss:  1.1854370832443237\n",
      "Epoch 15  batch 201 . Training Loss:  1.208063006401062\n",
      "Epoch 15: Validation Loss: 0.7305, Validation Accuracy: 68.01%\n",
      "Epoch 16  batch 1 . Training Loss:  0.9362819790840149\n",
      "Epoch 16  batch 101 . Training Loss:  0.8636797070503235\n",
      "Epoch 16  batch 201 . Training Loss:  1.135737419128418\n",
      "Epoch 16: Validation Loss: 0.6781, Validation Accuracy: 70.55%\n",
      "Epoch 17  batch 1 . Training Loss:  0.822738766670227\n",
      "Epoch 17  batch 101 . Training Loss:  1.2686262130737305\n",
      "Epoch 17  batch 201 . Training Loss:  0.9123320579528809\n",
      "Epoch 17: Validation Loss: 0.6475, Validation Accuracy: 69.85%\n",
      "Epoch 18  batch 1 . Training Loss:  0.7616688013076782\n",
      "Epoch 18  batch 101 . Training Loss:  0.874928891658783\n",
      "Epoch 18  batch 201 . Training Loss:  0.7057041525840759\n",
      "Epoch 18: Validation Loss: 0.6257, Validation Accuracy: 70.25%\n",
      "Epoch 19  batch 1 . Training Loss:  0.8116475939750671\n",
      "Epoch 19  batch 101 . Training Loss:  0.9432795643806458\n",
      "Epoch 19  batch 201 . Training Loss:  0.9785894751548767\n",
      "Epoch 19: Validation Loss: 0.6173, Validation Accuracy: 72.86%\n",
      "Epoch 20  batch 1 . Training Loss:  0.7649129033088684\n",
      "Epoch 20  batch 101 . Training Loss:  0.7156556248664856\n",
      "Epoch 20  batch 201 . Training Loss:  0.7857168912887573\n",
      "Epoch 20: Validation Loss: 0.5850, Validation Accuracy: 71.78%\n",
      "Epoch 21  batch 1 . Training Loss:  0.9368995428085327\n",
      "Epoch 21  batch 101 . Training Loss:  0.9708372354507446\n",
      "Epoch 21  batch 201 . Training Loss:  0.8071417808532715\n",
      "Epoch 21: Validation Loss: 0.5150, Validation Accuracy: 76.34%\n",
      "Epoch 22  batch 1 . Training Loss:  0.7977582812309265\n",
      "Epoch 22  batch 101 . Training Loss:  0.8341062068939209\n",
      "Epoch 22  batch 201 . Training Loss:  0.7561854124069214\n",
      "Epoch 22: Validation Loss: 0.5044, Validation Accuracy: 76.71%\n",
      "Epoch 23  batch 1 . Training Loss:  0.8060142397880554\n",
      "Epoch 23  batch 101 . Training Loss:  0.7113621234893799\n",
      "Epoch 23  batch 201 . Training Loss:  0.7307001948356628\n",
      "Epoch 23: Validation Loss: 0.5112, Validation Accuracy: 77.31%\n",
      "Epoch 24  batch 1 . Training Loss:  1.202976107597351\n",
      "Epoch 24  batch 101 . Training Loss:  0.7686097621917725\n",
      "Epoch 24  batch 201 . Training Loss:  0.8339942693710327\n",
      "Epoch 24: Validation Loss: 0.5035, Validation Accuracy: 76.33%\n",
      "Epoch 25  batch 1 . Training Loss:  0.7957324385643005\n",
      "Epoch 25  batch 101 . Training Loss:  0.8234708905220032\n",
      "Epoch 25  batch 201 . Training Loss:  0.7191283106803894\n",
      "Epoch 25: Validation Loss: 0.4820, Validation Accuracy: 76.80%\n",
      "Epoch 26  batch 1 . Training Loss:  0.7894259691238403\n",
      "Epoch 26  batch 101 . Training Loss:  0.577778697013855\n",
      "Epoch 26  batch 201 . Training Loss:  0.7147238850593567\n",
      "Epoch 26: Validation Loss: 0.4572, Validation Accuracy: 78.45%\n",
      "Epoch 27  batch 1 . Training Loss:  0.5425071716308594\n",
      "Epoch 27  batch 101 . Training Loss:  0.6574363112449646\n",
      "Epoch 27  batch 201 . Training Loss:  0.9101371169090271\n",
      "Epoch 27: Validation Loss: 0.4500, Validation Accuracy: 79.33%\n",
      "Epoch 28  batch 1 . Training Loss:  0.6179391145706177\n",
      "Epoch 28  batch 101 . Training Loss:  0.8897379040718079\n",
      "Epoch 28  batch 201 . Training Loss:  0.6513150930404663\n",
      "Epoch 28: Validation Loss: 0.4200, Validation Accuracy: 79.34%\n",
      "Epoch 29  batch 1 . Training Loss:  0.8141284584999084\n",
      "Epoch 29  batch 101 . Training Loss:  0.6459938883781433\n",
      "Epoch 29  batch 201 . Training Loss:  0.5635413527488708\n",
      "Epoch 29: Validation Loss: 0.4289, Validation Accuracy: 79.84%\n",
      "Epoch 30  batch 1 . Training Loss:  0.5357470512390137\n",
      "Epoch 30  batch 101 . Training Loss:  0.7428600788116455\n",
      "Epoch 30  batch 201 . Training Loss:  0.6015947461128235\n",
      "Epoch 30: Validation Loss: 0.3973, Validation Accuracy: 81.11%\n",
      "Epoch 31  batch 1 . Training Loss:  0.6025856733322144\n",
      "Epoch 31  batch 101 . Training Loss:  0.6282156109809875\n",
      "Epoch 31  batch 201 . Training Loss:  0.7736461758613586\n",
      "Epoch 31: Validation Loss: 0.3988, Validation Accuracy: 80.68%\n",
      "Epoch 32  batch 1 . Training Loss:  0.49859821796417236\n",
      "Epoch 32  batch 101 . Training Loss:  0.5209457278251648\n",
      "Epoch 32  batch 201 . Training Loss:  0.636969268321991\n",
      "Epoch 32: Validation Loss: 0.3732, Validation Accuracy: 80.68%\n",
      "Epoch 33  batch 1 . Training Loss:  0.4325016140937805\n",
      "Epoch 33  batch 101 . Training Loss:  0.6626766324043274\n",
      "Epoch 33  batch 201 . Training Loss:  0.8127071857452393\n",
      "Epoch 33: Validation Loss: 0.3740, Validation Accuracy: 80.92%\n",
      "Epoch 34  batch 1 . Training Loss:  0.5739108920097351\n",
      "Epoch 34  batch 101 . Training Loss:  0.8786914944648743\n",
      "Epoch 34  batch 201 . Training Loss:  0.4279607832431793\n",
      "Epoch 34: Validation Loss: 0.3583, Validation Accuracy: 82.26%\n",
      "Epoch 35  batch 1 . Training Loss:  0.4533965289592743\n",
      "Epoch 35  batch 101 . Training Loss:  0.6602651476860046\n",
      "Epoch 35  batch 201 . Training Loss:  0.48467162251472473\n",
      "Epoch 35: Validation Loss: 0.3540, Validation Accuracy: 82.97%\n",
      "Epoch 36  batch 1 . Training Loss:  0.5666436553001404\n",
      "Epoch 36  batch 101 . Training Loss:  0.5169387459754944\n",
      "Epoch 36  batch 201 . Training Loss:  0.7255486845970154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Validation Loss: 0.3419, Validation Accuracy: 82.68%\n",
      "Epoch 37  batch 1 . Training Loss:  0.629709780216217\n",
      "Epoch 37  batch 101 . Training Loss:  0.7041293382644653\n",
      "Epoch 37  batch 201 . Training Loss:  0.5941136479377747\n",
      "Epoch 37: Validation Loss: 0.3293, Validation Accuracy: 83.03%\n",
      "Epoch 38  batch 1 . Training Loss:  0.5397292971611023\n",
      "Epoch 38  batch 101 . Training Loss:  0.4399472773075104\n",
      "Epoch 38  batch 201 . Training Loss:  0.5783757567405701\n",
      "Epoch 38: Validation Loss: 0.3276, Validation Accuracy: 83.34%\n",
      "Epoch 39  batch 1 . Training Loss:  0.4393775761127472\n",
      "Epoch 39  batch 101 . Training Loss:  0.7997291684150696\n",
      "Epoch 39  batch 201 . Training Loss:  0.43415191769599915\n",
      "Epoch 39: Validation Loss: 0.3153, Validation Accuracy: 83.28%\n",
      "Epoch 40  batch 1 . Training Loss:  0.6963233947753906\n",
      "Epoch 40  batch 101 . Training Loss:  0.6284853219985962\n",
      "Epoch 40  batch 201 . Training Loss:  0.402750700712204\n",
      "Epoch 40: Validation Loss: 0.3017, Validation Accuracy: 85.52%\n",
      "Epoch 41  batch 1 . Training Loss:  0.5771352648735046\n",
      "Epoch 41  batch 101 . Training Loss:  0.5024183988571167\n",
      "Epoch 41  batch 201 . Training Loss:  0.6411054730415344\n",
      "Epoch 41: Validation Loss: 0.2949, Validation Accuracy: 84.44%\n",
      "Epoch 42  batch 1 . Training Loss:  0.6594929695129395\n",
      "Epoch 42  batch 101 . Training Loss:  0.5152023434638977\n",
      "Epoch 42  batch 201 . Training Loss:  0.3927680552005768\n",
      "Epoch 42: Validation Loss: 0.2810, Validation Accuracy: 85.00%\n",
      "Epoch 43  batch 1 . Training Loss:  0.5581358075141907\n",
      "Epoch 43  batch 101 . Training Loss:  0.42878738045692444\n",
      "Epoch 43  batch 201 . Training Loss:  0.6354280114173889\n",
      "Epoch 43: Validation Loss: 0.2916, Validation Accuracy: 84.15%\n",
      "Epoch 44  batch 1 . Training Loss:  0.5532717108726501\n",
      "Epoch 44  batch 101 . Training Loss:  0.7373723983764648\n",
      "Epoch 44  batch 201 . Training Loss:  0.33932921290397644\n",
      "Epoch 44: Validation Loss: 0.2852, Validation Accuracy: 85.21%\n",
      "Epoch 45  batch 1 . Training Loss:  0.49956977367401123\n",
      "Epoch 45  batch 101 . Training Loss:  0.5536851286888123\n",
      "Epoch 45  batch 201 . Training Loss:  0.5085654258728027\n",
      "Epoch 45: Validation Loss: 0.2798, Validation Accuracy: 85.44%\n",
      "Epoch 46  batch 1 . Training Loss:  0.45271456241607666\n",
      "Epoch 46  batch 101 . Training Loss:  0.4511162042617798\n",
      "Epoch 46  batch 201 . Training Loss:  0.43515774607658386\n",
      "Epoch 46: Validation Loss: 0.2573, Validation Accuracy: 86.95%\n",
      "Epoch 47  batch 1 . Training Loss:  0.515126645565033\n",
      "Epoch 47  batch 101 . Training Loss:  0.45473018288612366\n",
      "Epoch 47  batch 201 . Training Loss:  0.41209864616394043\n",
      "Epoch 47: Validation Loss: 0.2632, Validation Accuracy: 86.25%\n",
      "Epoch 48  batch 1 . Training Loss:  0.3914150297641754\n",
      "Epoch 48  batch 101 . Training Loss:  0.36828669905662537\n",
      "Epoch 48  batch 201 . Training Loss:  0.2850566804409027\n",
      "Epoch 48: Validation Loss: 0.2645, Validation Accuracy: 86.93%\n",
      "Epoch 49  batch 1 . Training Loss:  0.6221378445625305\n",
      "Epoch 49  batch 101 . Training Loss:  0.3566179871559143\n",
      "Epoch 49  batch 201 . Training Loss:  0.3969474732875824\n",
      "Epoch 49: Validation Loss: 0.2539, Validation Accuracy: 86.85%\n",
      "Epoch 50  batch 1 . Training Loss:  0.3676316738128662\n",
      "Epoch 50  batch 101 . Training Loss:  0.27812421321868896\n",
      "Epoch 50  batch 201 . Training Loss:  0.41579103469848633\n",
      "Epoch 50: Validation Loss: 0.2400, Validation Accuracy: 87.25%\n",
      "Epoch 51  batch 1 . Training Loss:  0.2739897668361664\n",
      "Epoch 51  batch 101 . Training Loss:  0.3704197406768799\n",
      "Epoch 51  batch 201 . Training Loss:  0.49768829345703125\n",
      "Epoch 51: Validation Loss: 0.2443, Validation Accuracy: 87.25%\n",
      "Epoch 52  batch 1 . Training Loss:  0.49642834067344666\n",
      "Epoch 52  batch 101 . Training Loss:  0.36523881554603577\n",
      "Epoch 52  batch 201 . Training Loss:  0.28134065866470337\n",
      "Epoch 52: Validation Loss: 0.2470, Validation Accuracy: 88.27%\n",
      "Epoch 53  batch 1 . Training Loss:  0.29272136092185974\n",
      "Epoch 53  batch 101 . Training Loss:  0.6372760534286499\n",
      "Epoch 53  batch 201 . Training Loss:  0.5079029202461243\n",
      "Epoch 53: Validation Loss: 0.2298, Validation Accuracy: 87.81%\n",
      "Epoch 54  batch 1 . Training Loss:  0.3327319324016571\n",
      "Epoch 54  batch 101 . Training Loss:  0.7272286415100098\n",
      "Epoch 54  batch 201 . Training Loss:  0.7296207547187805\n",
      "Epoch 54: Validation Loss: 0.2237, Validation Accuracy: 88.09%\n",
      "Epoch 55  batch 1 . Training Loss:  0.359918475151062\n",
      "Epoch 55  batch 101 . Training Loss:  0.5683006644248962\n",
      "Epoch 55  batch 201 . Training Loss:  0.4346923828125\n",
      "Epoch 55: Validation Loss: 0.2266, Validation Accuracy: 88.65%\n",
      "Epoch 56  batch 1 . Training Loss:  0.40638986229896545\n",
      "Epoch 56  batch 101 . Training Loss:  0.5564191341400146\n",
      "Epoch 56  batch 201 . Training Loss:  0.43431445956230164\n",
      "Epoch 56: Validation Loss: 0.2203, Validation Accuracy: 88.34%\n",
      "Epoch 57  batch 1 . Training Loss:  0.33000537753105164\n",
      "Epoch 57  batch 101 . Training Loss:  0.4917154014110565\n",
      "Epoch 57  batch 201 . Training Loss:  0.417368620634079\n",
      "Epoch 57: Validation Loss: 0.2083, Validation Accuracy: 88.85%\n",
      "Epoch 58  batch 1 . Training Loss:  0.2870306968688965\n",
      "Epoch 58  batch 101 . Training Loss:  0.4723410904407501\n",
      "Epoch 58  batch 201 . Training Loss:  0.4630815088748932\n",
      "Epoch 58: Validation Loss: 0.2067, Validation Accuracy: 88.73%\n",
      "Epoch 59  batch 1 . Training Loss:  0.24791787564754486\n",
      "Epoch 59  batch 101 . Training Loss:  0.43286216259002686\n",
      "Epoch 59  batch 201 . Training Loss:  0.34018561244010925\n",
      "Epoch 59: Validation Loss: 0.2126, Validation Accuracy: 89.26%\n",
      "Epoch 60  batch 1 . Training Loss:  0.30910801887512207\n",
      "Epoch 60  batch 101 . Training Loss:  0.5854955911636353\n",
      "Epoch 60  batch 201 . Training Loss:  0.27721545100212097\n",
      "Epoch 60: Validation Loss: 0.2211, Validation Accuracy: 88.97%\n",
      "Epoch 61  batch 1 . Training Loss:  0.3286607265472412\n",
      "Epoch 61  batch 101 . Training Loss:  0.45384812355041504\n",
      "Epoch 61  batch 201 . Training Loss:  0.356112539768219\n",
      "Epoch 61: Validation Loss: 0.2078, Validation Accuracy: 89.27%\n",
      "Epoch 62  batch 1 . Training Loss:  0.37701910734176636\n",
      "Epoch 62  batch 101 . Training Loss:  0.24544399976730347\n",
      "Epoch 62  batch 201 . Training Loss:  0.32520583271980286\n",
      "Epoch 62: Validation Loss: 0.1997, Validation Accuracy: 89.38%\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c366cb6eac245f682f7a264c521b088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.032 MB of 0.032 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▆▅▄▃▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▂▂▂▁▂▁▁▁▂▁▂▁</td></tr><tr><td>val_accuracy</td><td>▁▃▃▄▅▅▆▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.32521</td></tr><tr><td>train_loss</td><td>0.6436</td></tr><tr><td>val_accuracy</td><td>89.37636</td></tr><tr><td>val_loss</td><td>0.19973</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_v5</strong> at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/7glnky47' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/7glnky47</a><br/> View project at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_165937-7glnky47/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = net4\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %100 == 0:\n",
    "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
    "          if enable_wandb:\n",
    "            wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validation_loss = get_loss(val_loader, model, criterion, device)\n",
    "    validation_accuracy = get_accuracy(val_loader, model, device)\n",
    "    wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "    if early_stopping.early_stop(validation_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model - 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epochs = 50\n",
    "- Batch size = 128\n",
    "- Learning Rates = 0.001\n",
    "- Data Agumentation\n",
    "- Class Balancing \n",
    "- Early Stopping Patience 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/evafan/wandb/run-20240419_180207-vtuato2u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vtuato2u' target=\"_blank\">experiment_v6</a></strong> to <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vtuato2u' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/vtuato2u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vtuato2u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14d0094d7280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Project CNN_v1\", name = 'experiment_v6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_1 = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), \n",
    "    transforms.RandomHorizontalFlip(0.5), \n",
    "    transforms.RandomRotation(45), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), (0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "dataset_train = ImageFolder(\n",
    "    'Train',\n",
    "    transform = train_transform_1)\n",
    "\n",
    "train_data, val_data = random_split(dataset_train, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(dataset_test, shuffle=True, batch_size = 128, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout2d(p=0.3, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Dropout2d(p=0.4, inplace=False)\n",
       "    (14): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1000, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=256, out_features=43, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net5 = Net(num_classes=43)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_normalized = [weight * len(sorted_train) / sum(weights) for weight in weights]\n",
    "class_weights = torch.tensor(weights_normalized, dtype=torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "    class_weights = class_weights.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net5.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopper(patience=7,delta = 0.01)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.8933286666870117\n",
      "Epoch 1  batch 101 . Training Loss:  3.487985134124756\n",
      "Epoch 1  batch 201 . Training Loss:  3.0350165367126465\n",
      "Epoch 1: Validation Loss: 2.7386, Validation Accuracy: 9.04%\n",
      "Epoch 2  batch 1 . Training Loss:  2.8283674716949463\n",
      "Epoch 2  batch 101 . Training Loss:  2.5068910121917725\n",
      "Epoch 2  batch 201 . Training Loss:  2.461008310317993\n",
      "Epoch 2: Validation Loss: 2.2643, Validation Accuracy: 23.79%\n",
      "Epoch 3  batch 1 . Training Loss:  2.357834577560425\n",
      "Epoch 3  batch 101 . Training Loss:  2.249356269836426\n",
      "Epoch 3  batch 201 . Training Loss:  2.4024529457092285\n",
      "Epoch 3: Validation Loss: 2.0437, Validation Accuracy: 32.98%\n",
      "Epoch 4  batch 1 . Training Loss:  2.1649677753448486\n",
      "Epoch 4  batch 101 . Training Loss:  2.267047882080078\n",
      "Epoch 4  batch 201 . Training Loss:  1.7670679092407227\n",
      "Epoch 4: Validation Loss: 1.7687, Validation Accuracy: 35.02%\n",
      "Epoch 5  batch 1 . Training Loss:  2.073256731033325\n",
      "Epoch 5  batch 101 . Training Loss:  2.013406991958618\n",
      "Epoch 5  batch 201 . Training Loss:  1.7351325750350952\n",
      "Epoch 5: Validation Loss: 1.6225, Validation Accuracy: 39.61%\n",
      "Epoch 6  batch 1 . Training Loss:  1.8103820085525513\n",
      "Epoch 6  batch 101 . Training Loss:  1.8085758686065674\n",
      "Epoch 6  batch 201 . Training Loss:  1.5275098085403442\n",
      "Epoch 6: Validation Loss: 1.4739, Validation Accuracy: 42.55%\n",
      "Epoch 7  batch 1 . Training Loss:  1.8421608209609985\n",
      "Epoch 7  batch 101 . Training Loss:  1.797104835510254\n",
      "Epoch 7  batch 201 . Training Loss:  1.62933349609375\n",
      "Epoch 7: Validation Loss: 1.3358, Validation Accuracy: 46.78%\n",
      "Epoch 8  batch 1 . Training Loss:  1.7078404426574707\n",
      "Epoch 8  batch 101 . Training Loss:  1.480624794960022\n",
      "Epoch 8  batch 201 . Training Loss:  1.743942379951477\n",
      "Epoch 8: Validation Loss: 1.2650, Validation Accuracy: 49.09%\n",
      "Epoch 9  batch 1 . Training Loss:  1.313990592956543\n",
      "Epoch 9  batch 101 . Training Loss:  1.5314176082611084\n",
      "Epoch 9  batch 201 . Training Loss:  1.520411729812622\n",
      "Epoch 9: Validation Loss: 1.1475, Validation Accuracy: 51.51%\n",
      "Epoch 10  batch 1 . Training Loss:  1.5229980945587158\n",
      "Epoch 10  batch 101 . Training Loss:  1.3000376224517822\n",
      "Epoch 10  batch 201 . Training Loss:  1.1776421070098877\n",
      "Epoch 10: Validation Loss: 1.0892, Validation Accuracy: 53.77%\n",
      "Epoch 11  batch 1 . Training Loss:  1.3685611486434937\n",
      "Epoch 11  batch 101 . Training Loss:  1.3136128187179565\n",
      "Epoch 11  batch 201 . Training Loss:  1.2252600193023682\n",
      "Epoch 11: Validation Loss: 1.1120, Validation Accuracy: 53.64%\n",
      "Epoch 12  batch 1 . Training Loss:  1.2219585180282593\n",
      "Epoch 12  batch 101 . Training Loss:  1.079017996788025\n",
      "Epoch 12  batch 201 . Training Loss:  1.1198700666427612\n",
      "Epoch 12: Validation Loss: 0.9642, Validation Accuracy: 57.21%\n",
      "Epoch 13  batch 1 . Training Loss:  1.2949780225753784\n",
      "Epoch 13  batch 101 . Training Loss:  0.9719895720481873\n",
      "Epoch 13  batch 201 . Training Loss:  1.1573859453201294\n",
      "Epoch 13: Validation Loss: 0.8856, Validation Accuracy: 59.07%\n",
      "Epoch 14  batch 1 . Training Loss:  0.9911688566207886\n",
      "Epoch 14  batch 101 . Training Loss:  1.1792323589324951\n",
      "Epoch 14  batch 201 . Training Loss:  1.1650309562683105\n",
      "Epoch 14: Validation Loss: 0.8501, Validation Accuracy: 62.66%\n",
      "Epoch 15  batch 1 . Training Loss:  1.131832242012024\n",
      "Epoch 15  batch 101 . Training Loss:  1.3182573318481445\n",
      "Epoch 15  batch 201 . Training Loss:  1.2452857494354248\n",
      "Epoch 15: Validation Loss: 0.7783, Validation Accuracy: 63.49%\n",
      "Epoch 16  batch 1 . Training Loss:  0.9916082620620728\n",
      "Epoch 16  batch 101 . Training Loss:  1.2438182830810547\n",
      "Epoch 16  batch 201 . Training Loss:  0.9856945872306824\n",
      "Epoch 16: Validation Loss: 0.7310, Validation Accuracy: 63.24%\n",
      "Epoch 17  batch 1 . Training Loss:  0.9073716998100281\n",
      "Epoch 17  batch 101 . Training Loss:  1.0467723608016968\n",
      "Epoch 17  batch 201 . Training Loss:  1.133931279182434\n",
      "Epoch 17: Validation Loss: 0.7280, Validation Accuracy: 63.35%\n",
      "Epoch 18  batch 1 . Training Loss:  1.0182783603668213\n",
      "Epoch 18  batch 101 . Training Loss:  0.8625841736793518\n",
      "Epoch 18  batch 201 . Training Loss:  1.1141959428787231\n",
      "Epoch 18: Validation Loss: 0.6534, Validation Accuracy: 68.54%\n",
      "Epoch 19  batch 1 . Training Loss:  1.1180119514465332\n",
      "Epoch 19  batch 101 . Training Loss:  0.8253533840179443\n",
      "Epoch 19  batch 201 . Training Loss:  0.9489070177078247\n",
      "Epoch 19: Validation Loss: 0.6060, Validation Accuracy: 70.81%\n",
      "Epoch 20  batch 1 . Training Loss:  0.8464421033859253\n",
      "Epoch 20  batch 101 . Training Loss:  0.9976266026496887\n",
      "Epoch 20  batch 201 . Training Loss:  0.902538001537323\n",
      "Epoch 20: Validation Loss: 0.6049, Validation Accuracy: 70.46%\n",
      "Epoch 21  batch 1 . Training Loss:  0.8408732414245605\n",
      "Epoch 21  batch 101 . Training Loss:  1.0789295434951782\n",
      "Epoch 21  batch 201 . Training Loss:  0.7406579852104187\n",
      "Epoch 21: Validation Loss: 0.6066, Validation Accuracy: 71.06%\n",
      "Epoch 22  batch 1 . Training Loss:  0.7764317989349365\n",
      "Epoch 22  batch 101 . Training Loss:  0.6776530146598816\n",
      "Epoch 22  batch 201 . Training Loss:  1.1039124727249146\n",
      "Epoch 22: Validation Loss: 0.5532, Validation Accuracy: 70.65%\n",
      "Epoch 23  batch 1 . Training Loss:  0.9026547074317932\n",
      "Epoch 23  batch 101 . Training Loss:  0.76783287525177\n",
      "Epoch 23  batch 201 . Training Loss:  0.6748819351196289\n",
      "Epoch 23: Validation Loss: 0.5572, Validation Accuracy: 72.26%\n",
      "Epoch 24  batch 1 . Training Loss:  0.7359688878059387\n",
      "Epoch 24  batch 101 . Training Loss:  0.85833740234375\n",
      "Epoch 24  batch 201 . Training Loss:  0.8189914226531982\n",
      "Epoch 24: Validation Loss: 0.5057, Validation Accuracy: 73.65%\n",
      "Epoch 25  batch 1 . Training Loss:  0.6705139875411987\n",
      "Epoch 25  batch 101 . Training Loss:  0.6394672989845276\n",
      "Epoch 25  batch 201 . Training Loss:  0.6518832445144653\n",
      "Epoch 25: Validation Loss: 0.4865, Validation Accuracy: 75.44%\n",
      "Epoch 26  batch 1 . Training Loss:  0.8245112299919128\n",
      "Epoch 26  batch 101 . Training Loss:  1.0195521116256714\n",
      "Epoch 26  batch 201 . Training Loss:  0.8940483331680298\n",
      "Epoch 26: Validation Loss: 0.4618, Validation Accuracy: 76.36%\n",
      "Epoch 27  batch 1 . Training Loss:  0.9347239136695862\n",
      "Epoch 27  batch 101 . Training Loss:  0.6942223906517029\n",
      "Epoch 27  batch 201 . Training Loss:  0.5574149489402771\n",
      "Epoch 27: Validation Loss: 0.4672, Validation Accuracy: 76.51%\n",
      "Epoch 28  batch 1 . Training Loss:  0.639135479927063\n",
      "Epoch 28  batch 101 . Training Loss:  0.7648078799247742\n",
      "Epoch 28  batch 201 . Training Loss:  0.6141964197158813\n",
      "Epoch 28: Validation Loss: 0.4706, Validation Accuracy: 78.08%\n",
      "Epoch 29  batch 1 . Training Loss:  0.5873894691467285\n",
      "Epoch 29  batch 101 . Training Loss:  0.6982563734054565\n",
      "Epoch 29  batch 201 . Training Loss:  0.742380678653717\n",
      "Epoch 29: Validation Loss: 0.4164, Validation Accuracy: 78.66%\n",
      "Epoch 30  batch 1 . Training Loss:  0.5102918744087219\n",
      "Epoch 30  batch 101 . Training Loss:  0.6556260585784912\n",
      "Epoch 30  batch 201 . Training Loss:  0.58708256483078\n",
      "Epoch 30: Validation Loss: 0.4243, Validation Accuracy: 80.13%\n",
      "Epoch 31  batch 1 . Training Loss:  0.7388165593147278\n",
      "Epoch 31  batch 101 . Training Loss:  0.7257134318351746\n",
      "Epoch 31  batch 201 . Training Loss:  0.6454238295555115\n",
      "Epoch 31: Validation Loss: 0.3998, Validation Accuracy: 79.70%\n",
      "Epoch 32  batch 1 . Training Loss:  0.7705849409103394\n",
      "Epoch 32  batch 101 . Training Loss:  0.6252129673957825\n",
      "Epoch 32  batch 201 . Training Loss:  0.5412643551826477\n",
      "Epoch 32: Validation Loss: 0.3897, Validation Accuracy: 80.73%\n",
      "Epoch 33  batch 1 . Training Loss:  0.581987738609314\n",
      "Epoch 33  batch 101 . Training Loss:  0.521580696105957\n",
      "Epoch 33  batch 201 . Training Loss:  0.5841583013534546\n",
      "Epoch 33: Validation Loss: 0.3886, Validation Accuracy: 79.02%\n",
      "Epoch 34  batch 1 . Training Loss:  0.7022942304611206\n",
      "Epoch 34  batch 101 . Training Loss:  0.5304099917411804\n",
      "Epoch 34  batch 201 . Training Loss:  0.5795632004737854\n",
      "Epoch 34: Validation Loss: 0.3779, Validation Accuracy: 80.37%\n",
      "Epoch 35  batch 1 . Training Loss:  0.569948673248291\n",
      "Epoch 35  batch 101 . Training Loss:  0.7760286927223206\n",
      "Epoch 35  batch 201 . Training Loss:  0.6545551419258118\n",
      "Epoch 35: Validation Loss: 0.3640, Validation Accuracy: 80.64%\n",
      "Epoch 36  batch 1 . Training Loss:  0.5506989359855652\n",
      "Epoch 36  batch 101 . Training Loss:  0.8803251385688782\n",
      "Epoch 36  batch 201 . Training Loss:  0.6241892576217651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Validation Loss: 0.3379, Validation Accuracy: 82.82%\n",
      "Epoch 37  batch 1 . Training Loss:  0.48224759101867676\n",
      "Epoch 37  batch 101 . Training Loss:  0.5396406054496765\n",
      "Epoch 37  batch 201 . Training Loss:  0.5639005303382874\n",
      "Epoch 37: Validation Loss: 0.3288, Validation Accuracy: 82.83%\n",
      "Epoch 38  batch 1 . Training Loss:  0.5898976922035217\n",
      "Epoch 38  batch 101 . Training Loss:  0.5728935599327087\n",
      "Epoch 38  batch 201 . Training Loss:  0.5638312101364136\n",
      "Epoch 38: Validation Loss: 0.3251, Validation Accuracy: 82.74%\n",
      "Epoch 39  batch 1 . Training Loss:  0.6789456009864807\n",
      "Epoch 39  batch 101 . Training Loss:  0.7256856560707092\n",
      "Epoch 39  batch 201 . Training Loss:  0.582680881023407\n",
      "Epoch 39: Validation Loss: 0.3318, Validation Accuracy: 83.85%\n",
      "Epoch 40  batch 1 . Training Loss:  0.7096741199493408\n",
      "Epoch 40  batch 101 . Training Loss:  0.5494446754455566\n",
      "Epoch 40  batch 201 . Training Loss:  0.43946805596351624\n",
      "Epoch 40: Validation Loss: 0.3085, Validation Accuracy: 84.59%\n",
      "Epoch 41  batch 1 . Training Loss:  0.6134163737297058\n",
      "Epoch 41  batch 101 . Training Loss:  0.34024912118911743\n",
      "Epoch 41  batch 201 . Training Loss:  0.4383082985877991\n",
      "Epoch 41: Validation Loss: 0.2999, Validation Accuracy: 84.29%\n",
      "Epoch 42  batch 1 . Training Loss:  0.5114837288856506\n",
      "Epoch 42  batch 101 . Training Loss:  0.44978025555610657\n",
      "Epoch 42  batch 201 . Training Loss:  0.30910396575927734\n",
      "Epoch 42: Validation Loss: 0.2851, Validation Accuracy: 84.16%\n",
      "Epoch 43  batch 1 . Training Loss:  0.5963035821914673\n",
      "Epoch 43  batch 101 . Training Loss:  0.6283763647079468\n",
      "Epoch 43  batch 201 . Training Loss:  0.4082912504673004\n",
      "Epoch 43: Validation Loss: 0.3022, Validation Accuracy: 83.97%\n",
      "Epoch 44  batch 1 . Training Loss:  0.44156643748283386\n",
      "Epoch 44  batch 101 . Training Loss:  0.47432589530944824\n",
      "Epoch 44  batch 201 . Training Loss:  0.4186505973339081\n",
      "Epoch 44: Validation Loss: 0.2846, Validation Accuracy: 85.21%\n",
      "Epoch 45  batch 1 . Training Loss:  0.49435555934906006\n",
      "Epoch 45  batch 101 . Training Loss:  0.6322654485702515\n",
      "Epoch 45  batch 201 . Training Loss:  0.43442875146865845\n",
      "Epoch 45: Validation Loss: 0.2651, Validation Accuracy: 85.33%\n",
      "Epoch 46  batch 1 . Training Loss:  0.5704811811447144\n",
      "Epoch 46  batch 101 . Training Loss:  0.41315820813179016\n",
      "Epoch 46  batch 201 . Training Loss:  0.46562108397483826\n",
      "Epoch 46: Validation Loss: 0.2762, Validation Accuracy: 85.26%\n",
      "Epoch 47  batch 1 . Training Loss:  0.4653918147087097\n",
      "Epoch 47  batch 101 . Training Loss:  0.36504727602005005\n",
      "Epoch 47  batch 201 . Training Loss:  0.4394806921482086\n",
      "Epoch 47: Validation Loss: 0.2530, Validation Accuracy: 86.47%\n",
      "Epoch 48  batch 1 . Training Loss:  0.4488263726234436\n",
      "Epoch 48  batch 101 . Training Loss:  0.45432546734809875\n",
      "Epoch 48  batch 201 . Training Loss:  0.3470575511455536\n",
      "Epoch 48: Validation Loss: 0.2941, Validation Accuracy: 84.49%\n",
      "Epoch 49  batch 1 . Training Loss:  0.8089742660522461\n",
      "Epoch 49  batch 101 . Training Loss:  0.6135194301605225\n",
      "Epoch 49  batch 201 . Training Loss:  0.4996672570705414\n",
      "Epoch 49: Validation Loss: 0.2489, Validation Accuracy: 86.39%\n",
      "Epoch 50  batch 1 . Training Loss:  0.46162328124046326\n",
      "Epoch 50  batch 101 . Training Loss:  0.3294932544231415\n",
      "Epoch 50  batch 201 . Training Loss:  0.3531564474105835\n",
      "Epoch 50: Validation Loss: 0.2544, Validation Accuracy: 85.69%\n",
      "Epoch 51  batch 1 . Training Loss:  0.5452011823654175\n",
      "Epoch 51  batch 101 . Training Loss:  0.668560802936554\n",
      "Epoch 51  batch 201 . Training Loss:  0.48212388157844543\n",
      "Epoch 51: Validation Loss: 0.2465, Validation Accuracy: 85.81%\n",
      "Epoch 52  batch 1 . Training Loss:  0.572134256362915\n",
      "Epoch 52  batch 101 . Training Loss:  0.6205698847770691\n",
      "Epoch 52  batch 201 . Training Loss:  0.5202555656433105\n",
      "Epoch 52: Validation Loss: 0.2368, Validation Accuracy: 86.94%\n",
      "Epoch 53  batch 1 . Training Loss:  0.41817182302474976\n",
      "Epoch 53  batch 101 . Training Loss:  0.4764518141746521\n",
      "Epoch 53  batch 201 . Training Loss:  0.3753989338874817\n",
      "Epoch 53: Validation Loss: 0.2357, Validation Accuracy: 86.56%\n",
      "Epoch 54  batch 1 . Training Loss:  0.41985803842544556\n",
      "Epoch 54  batch 101 . Training Loss:  0.48858770728111267\n",
      "Epoch 54  batch 201 . Training Loss:  0.32446327805519104\n",
      "Epoch 54: Validation Loss: 0.2358, Validation Accuracy: 87.07%\n",
      "Epoch 55  batch 1 . Training Loss:  0.5524875521659851\n",
      "Epoch 55  batch 101 . Training Loss:  0.45406216382980347\n",
      "Epoch 55  batch 201 . Training Loss:  0.4458305239677429\n",
      "Epoch 55: Validation Loss: 0.2536, Validation Accuracy: 86.57%\n",
      "Epoch 56  batch 1 . Training Loss:  0.48546719551086426\n",
      "Epoch 56  batch 101 . Training Loss:  0.368844598531723\n",
      "Epoch 56  batch 201 . Training Loss:  0.4834800362586975\n",
      "Epoch 56: Validation Loss: 0.2300, Validation Accuracy: 88.14%\n",
      "Epoch 57  batch 1 . Training Loss:  0.5701488852500916\n",
      "Epoch 57  batch 101 . Training Loss:  0.46608734130859375\n",
      "Epoch 57  batch 201 . Training Loss:  0.3758268654346466\n",
      "Epoch 57: Validation Loss: 0.2270, Validation Accuracy: 87.63%\n",
      "Epoch 58  batch 1 . Training Loss:  0.4303678274154663\n",
      "Epoch 58  batch 101 . Training Loss:  0.3825621008872986\n",
      "Epoch 58  batch 201 . Training Loss:  0.3725324869155884\n",
      "Epoch 58: Validation Loss: 0.2202, Validation Accuracy: 88.19%\n",
      "Epoch 59  batch 1 . Training Loss:  0.3178809583187103\n",
      "Epoch 59  batch 101 . Training Loss:  0.6492804884910583\n",
      "Epoch 59  batch 201 . Training Loss:  0.4206717312335968\n",
      "Epoch 59: Validation Loss: 0.2077, Validation Accuracy: 88.08%\n",
      "Epoch 60  batch 1 . Training Loss:  0.39556893706321716\n",
      "Epoch 60  batch 101 . Training Loss:  0.3776470124721527\n",
      "Epoch 60  batch 201 . Training Loss:  0.45279601216316223\n",
      "Epoch 60: Validation Loss: 0.2116, Validation Accuracy: 88.06%\n",
      "Epoch 61  batch 1 . Training Loss:  0.4500146210193634\n",
      "Epoch 61  batch 101 . Training Loss:  0.30960485339164734\n",
      "Epoch 61  batch 201 . Training Loss:  0.44135499000549316\n",
      "Epoch 61: Validation Loss: 0.2149, Validation Accuracy: 87.97%\n",
      "Epoch 62  batch 1 . Training Loss:  0.3962472677230835\n",
      "Epoch 62  batch 101 . Training Loss:  0.3895207345485687\n",
      "Epoch 62  batch 201 . Training Loss:  0.2671279013156891\n",
      "Epoch 62: Validation Loss: 0.2098, Validation Accuracy: 88.60%\n",
      "Epoch 63  batch 1 . Training Loss:  0.47037068009376526\n",
      "Epoch 63  batch 101 . Training Loss:  0.4704485535621643\n",
      "Epoch 63  batch 201 . Training Loss:  0.42903417348861694\n",
      "Epoch 63: Validation Loss: 0.1879, Validation Accuracy: 89.06%\n",
      "Epoch 64  batch 1 . Training Loss:  0.3848489820957184\n",
      "Epoch 64  batch 101 . Training Loss:  0.5571913719177246\n",
      "Epoch 64  batch 201 . Training Loss:  0.26941195130348206\n",
      "Epoch 64: Validation Loss: 0.2012, Validation Accuracy: 88.19%\n",
      "Epoch 65  batch 1 . Training Loss:  0.28742432594299316\n",
      "Epoch 65  batch 101 . Training Loss:  0.35414883494377136\n",
      "Epoch 65  batch 201 . Training Loss:  0.3864723742008209\n",
      "Epoch 65: Validation Loss: 0.2025, Validation Accuracy: 88.66%\n",
      "Epoch 66  batch 1 . Training Loss:  0.4324811100959778\n",
      "Epoch 66  batch 101 . Training Loss:  0.49255070090293884\n",
      "Epoch 66  batch 201 . Training Loss:  0.3866811990737915\n",
      "Epoch 66: Validation Loss: 0.1877, Validation Accuracy: 88.92%\n",
      "Epoch 67  batch 1 . Training Loss:  0.3414969742298126\n",
      "Epoch 67  batch 101 . Training Loss:  0.6010718941688538\n",
      "Epoch 67  batch 201 . Training Loss:  0.39616572856903076\n",
      "Epoch 67: Validation Loss: 0.1912, Validation Accuracy: 88.42%\n",
      "Epoch 68  batch 1 . Training Loss:  0.44872644543647766\n",
      "Epoch 68  batch 101 . Training Loss:  0.3314562439918518\n",
      "Epoch 68  batch 201 . Training Loss:  0.306258887052536\n",
      "Epoch 68: Validation Loss: 0.1655, Validation Accuracy: 90.15%\n",
      "Epoch 69  batch 1 . Training Loss:  0.3639604151248932\n",
      "Epoch 69  batch 101 . Training Loss:  0.2525911033153534\n",
      "Epoch 69  batch 201 . Training Loss:  0.3575429618358612\n",
      "Epoch 69: Validation Loss: 0.1875, Validation Accuracy: 90.15%\n",
      "Epoch 70  batch 1 . Training Loss:  0.2859802544116974\n",
      "Epoch 70  batch 101 . Training Loss:  0.42927008867263794\n",
      "Epoch 70  batch 201 . Training Loss:  0.3522763252258301\n",
      "Epoch 70: Validation Loss: 0.1876, Validation Accuracy: 88.96%\n",
      "Epoch 71  batch 1 . Training Loss:  0.5773104429244995\n",
      "Epoch 71  batch 101 . Training Loss:  0.2803310751914978\n",
      "Epoch 71  batch 201 . Training Loss:  0.3395494520664215\n",
      "Epoch 71: Validation Loss: 0.1859, Validation Accuracy: 89.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72  batch 1 . Training Loss:  0.26299765706062317\n",
      "Epoch 72  batch 101 . Training Loss:  0.3608761131763458\n",
      "Epoch 72  batch 201 . Training Loss:  0.44342151284217834\n",
      "Epoch 72: Validation Loss: 0.1754, Validation Accuracy: 90.32%\n",
      "Epoch 73  batch 1 . Training Loss:  0.36048686504364014\n",
      "Epoch 73  batch 101 . Training Loss:  0.34961360692977905\n",
      "Epoch 73  batch 201 . Training Loss:  0.36577674746513367\n",
      "Epoch 73: Validation Loss: 0.1560, Validation Accuracy: 91.07%\n",
      "Epoch 74  batch 1 . Training Loss:  0.4020003080368042\n",
      "Epoch 74  batch 101 . Training Loss:  0.4022863209247589\n",
      "Epoch 74  batch 201 . Training Loss:  0.42391663789749146\n",
      "Epoch 74: Validation Loss: 0.1651, Validation Accuracy: 90.95%\n",
      "Epoch 75  batch 1 . Training Loss:  0.37245362997055054\n",
      "Epoch 75  batch 101 . Training Loss:  0.3831147849559784\n",
      "Epoch 75  batch 201 . Training Loss:  0.2977456748485565\n",
      "Epoch 75: Validation Loss: 0.1687, Validation Accuracy: 90.38%\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cd7988b8764a4a84b11fb1267f15a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▅▃▃▃▃▃▃▂▂▃▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▂</td></tr><tr><td>val_accuracy</td><td>▁▂▃▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇██████████████</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.29775</td></tr><tr><td>train_loss</td><td>0.38454</td></tr><tr><td>val_accuracy</td><td>90.38388</td></tr><tr><td>val_loss</td><td>0.1687</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_v6</strong> at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1/runs/vtuato2u' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1/runs/vtuato2u</a><br/> View project at: <a href='https://wandb.ai/evafan123/Project%20CNN_v1' target=\"_blank\">https://wandb.ai/evafan123/Project%20CNN_v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240419_180207-vtuato2u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "model = net5\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i %100 == 0:\n",
    "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
    "          if enable_wandb:\n",
    "            wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    validation_loss = get_loss(val_loader, model, criterion, device)\n",
    "    validation_accuracy = get_accuracy(val_loader, model, device)\n",
    "    wandb.log({\"val_loss\": validation_loss, \"val_accuracy\": validation_accuracy})\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "    if early_stopping.early_stop(validation_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources:\n",
    "- Generative AI was utilized for Debugging, code improvement, sentence structure and grammar.\n",
    "- https://github.com/poojahira/gtsrb-pytorch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "t1_mRxBrBq98"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
