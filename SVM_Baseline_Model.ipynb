{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Deep Learning for Deciphering Traffic Signs\n","# SVM Notebook\n","_________________________________________________________________________________________________________________________________________________________________________________\n","\n","##### Contributors:\n"," Victor Floriano, Yifan Fan, Jose Salerno"]},{"cell_type":"markdown","metadata":{},"source":["## Problem Statement & Motivation\n","As the world advances towards autonomous vehicles, our team has observed the remarkable efforts of large car manufacturers, who are working with data scientists to develop fully autonomous cars. Our team is excited to contribute to the development of this technology by creating a neural network model that will be able to classify different traffic signs. Our ultimate goal is to assist car makers in overcoming the challenges they may face in implementing neural network models that effectively read traffic signs and further their efforts toward a fully autonomous car or assisted driving. We believe autonomous driving to be an important problem to solve due to the great economic benefits it can generate for car manufacturers and the improvement of general driving safety.\n","\n","## Data Preparation\n"," We've selected the German Traffic Sign Recognition Benchmark (GTSRB) as our primary dataset. It's renowned for its complexity, featuring over 50,000 images across more than 40 classes of traffic signs. The GTSRB is publicly accessible through two resources. To efficiently manage the extensive and complex GTSRB dataset, our strategy integrates preprocessing for uniformity, data augmentation for robustness, and batch processing for computational efficiency. We'll employ distributed computing to parallelize operations, enhancing processing speed, and use stratified sampling for quick experimentation without compromising representativeness.\n","\n","\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# SVM - Baseline Model \n","\n"," \n","________________________________________________________________________________________________________________________________________________\n","\n","Results: \n","\n","    - "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27088,"status":"ok","timestamp":1713362535401,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"},"user_tz":180},"id":"0J1sZg4mXKoN","outputId":"c847fcbf-dc5b-4a81-c275-e6ddf357ba78"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bPelh25Y9Jx"},"outputs":[],"source":["#Runtime ~22min\n","import os\n","import cv2\n","import numpy as np\n","\n","def load_images_and_labels(base_path, max_images=100000):\n","    data = []\n","    labels = []\n","    image_count = 0  # Initialize a counter for the images\n","    classes = sorted(os.listdir(base_path))\n","    total_classes = len(classes)\n","\n","    for label, cls in enumerate(classes):\n","        cls_folder = os.path.join(base_path, cls)\n","        if os.path.isdir(cls_folder):\n","            image_files = os.listdir(cls_folder)\n","            total_images = len(image_files)\n","\n","            for idx, img_filename in enumerate(image_files):\n","                if image_count >= max_images:\n","                    print(\"Reached the maximum number of images to process.\")\n","                    return np.array(data), np.array(labels)  # Return the data collected so far\n","\n","                img_path = os.path.join(cls_folder, img_filename)\n","                img = cv2.imread(img_path)\n","                if img is not None:\n","                    img = cv2.resize(img, (32, 32))\n","                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","                    img = img.flatten() /255 #Flattens and normalizes the images\n","                    data.append(img)\n","                    labels.append(label)\n","                    image_count += 1  # Increment the counter\n","\n","                #Print progress for each class every 100 images\n","                if (idx + 1) % 1000 == 0 or idx == total_images - 1:\n","                    print(f\"Processed {idx + 1}/{total_images} images in class {label + 1}/{total_classes} ({cls})\")\n","\n","    return np.array(data), np.array(labels)\n","\n","\n","base_path = '/content/drive/MyDrive/BU_MSBA/BA865 - Neural Networks/BA865 - Group Project/GTSRBkaggle/Train'\n","\n","#Load data\n","train_data, train_labels = load_images_and_labels(base_path)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":661,"status":"ok","timestamp":1713367867614,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"},"user_tz":180},"id":"adwbK1QZZQ-D"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","#Split data into training and vaidation sets\n","X_train, X_validation, y_train, y_validation = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":545397,"status":"ok","timestamp":1713368415416,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"},"user_tz":180},"id":"S-kcfrMkZbnZ","outputId":"b781b0c3-2b25-4997-e443-761975f432eb"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div>"],"text/plain":["SVC(random_state=42)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["#Runtime ~9min\n","from sklearn.svm import SVC\n","\n","#Since your features are already scaled to [0,1] we skipped the StandardScaler()\n","svm_classifier = SVC(kernel='rbf', random_state=42) #rbf seems to be the standard kernell for image classification\n","\n","#Train the model on test data\n","svm_classifier.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1713368999727,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"},"user_tz":180},"id":"O3E5p3qhcnV5"},"outputs":[],"source":["#Runtime ~ 4min\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","#Genarate predictions on validation data\n","y_val_pred = svm_classifier.predict(X_validation)\n","\n","#Calculate accuracy\n","accuracy = accuracy_score(y_validation, y_val_pred)\n","print(f\"Validation Accuracy: {accuracy:.2f}\")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1713369003099,"user":{"displayName":"Victor Floriano","userId":"16962368443903150562"},"user_tz":180},"id":"losS6702v2k4","outputId":"ded79e9f-431c-4b16-cfd0-1d8aeffade6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report by Class:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      0.71      0.83        38\n","           1       0.91      0.89      0.90       496\n","           2       0.86      0.85      0.86       451\n","           3       0.69      0.73      0.71       281\n","           4       0.85      0.83      0.84       417\n","           5       0.67      0.75      0.71       357\n","           6       0.84      0.75      0.80        65\n","           7       0.90      0.77      0.83       254\n","           8       0.57      0.83      0.68       303\n","           9       0.99      0.89      0.94       276\n","          10       0.65      0.92      0.76       395\n","          11       0.89      0.92      0.90       252\n","          12       0.83      0.93      0.87       442\n","          13       0.98      0.96      0.97       457\n","          14       0.99      0.92      0.96       143\n","          15       0.94      0.87      0.90       108\n","          16       1.00      0.81      0.90        86\n","          17       0.99      0.91      0.95       217\n","          18       0.89      0.87      0.88       239\n","          19       1.00      0.47      0.64        32\n","          20       0.93      0.58      0.71        73\n","          21       0.96      0.74      0.84        66\n","          22       0.99      0.95      0.97        80\n","          23       0.97      0.82      0.89       109\n","          24       1.00      0.61      0.76        51\n","          25       0.91      0.94      0.92       312\n","          26       0.89      0.81      0.85       109\n","          27       0.97      0.58      0.73        48\n","          28       0.96      0.86      0.91       113\n","          29       0.96      0.84      0.90        58\n","          30       0.88      0.77      0.82       101\n","          31       0.85      0.82      0.83       157\n","          32       0.84      0.78      0.81        41\n","          33       0.99      0.90      0.94       138\n","          34       1.00      0.85      0.92        78\n","          35       1.00      0.92      0.96       234\n","          36       1.00      0.87      0.93        87\n","          37       1.00      0.93      0.97        45\n","          38       0.88      0.98      0.93       413\n","          39       1.00      0.74      0.85        53\n","          40       1.00      0.65      0.79        72\n","          41       0.97      0.78      0.87        46\n","          42       1.00      0.57      0.72        51\n","\n","    accuracy                           0.86      7844\n","   macro avg       0.92      0.81      0.85      7844\n","weighted avg       0.88      0.86      0.86      7844\n","\n"]}],"source":["#Check accuracy by class\n","class_report = classification_report(y_validation, y_val_pred)\n","print(\"Classification Report by Class:\\n\", class_report)"]},{"cell_type":"markdown","metadata":{},"source":["----"]},{"cell_type":"markdown","metadata":{},"source":["# Sources:\n","- Generative AI was utilized for Debugging, code improvement, sentence structure and grammar.\n","- \n","- \n","- "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNF5aonB+ico0C6eJMc2kiK","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
