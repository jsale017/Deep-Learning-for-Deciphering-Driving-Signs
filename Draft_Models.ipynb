{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKlU1qk4Zly7"
   },
   "source": [
    "# Deep Learning for Deciphering Traffic Signs\n",
    "# Draft Models Notebook\n",
    "_________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "##### Contributors:\n",
    " Victor Floriano, Yifan Fan, Jose Salerno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iFnzTGUdjrz"
   },
   "source": [
    "## Problem Statement & Motivation\n",
    "As the world advances towards autonomous vehicles, our team has observed the remarkable efforts of large car manufacturers, who are working with data scientists to develop fully autonomous cars. Our team is excited to contribute to the development of this technology by creating a neural network model that will be able to classify different traffic signs. Our ultimate goal is to assist car makers in overcoming the challenges they may face in implementing neural network models that effectively read traffic signs and further their efforts toward a fully autonomous car or assisted driving. We believe autonomous driving to be an important problem to solve due to the great economic benefits it can generate for car manufacturers and the improvement of general driving safety.\n",
    "\n",
    "## Data Preparation\n",
    " We've selected the German Traffic Sign Recognition Benchmark (GTSRB) as our primary dataset. It's renowned for its complexity, featuring over 50,000 images across more than 40 classes of traffic signs. The GTSRB is publicly accessible through two resources. To efficiently manage the extensive and complex GTSRB dataset, our strategy integrates preprocessing for uniformity, data augmentation for robustness, and batch processing for computational efficiency. We'll employ distributed computing to parallelize operations, enhancing processing speed, and use stratified sampling for quick experimentation without compromising representativeness.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft Models\n",
    "\n",
    "This notebook represents the initial phase of our project's model creation, focusing on experimenting with MLP (Multi-Layer Perceptron) and CNN (Convolutional Neural Network) models. The primary goal of this phase was to explore different model architectures and their performance in our specific use case. \n",
    "\n",
    "Due to the constraints of the Colab environment, this initial phase was limited in terms of computational resources. Despite these limitations, the insights gained from this exploration were instrumental in guiding our approach to model development.\n",
    "\n",
    "These are the final notebooks of our models:\n",
    "\n",
    "- MLP_Models.ipynb\n",
    "\n",
    "- CNN_Models.ipynb\n",
    "\n",
    "- SVM_Baseline_Model.ipynb\n",
    "__________________________________________________________________________________________________________________________________\n",
    "\n",
    "Results: \n",
    "- MLP initial Validation Accuracy: 0.39\n",
    "- CNN initial Validation Accuracy: 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXPh9xcKdsh3"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DaPgqKKKZlaG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvoyQffZ2-HM",
    "outputId": "8af2580a-504c-4e7e-d697-b06cb97c018b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96uEXO-WNvkS",
    "outputId": "82350ef1-0cac-4bb6-e317-27acfa3ae8a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr4/ba820/jsale017/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_wandb = True\n",
    "use_gpu = True\n",
    "gpu_available = torch.cuda.is_available()\n",
    "gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xcvmiWVbdlUg"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TjZBv1zw2_xN"
   },
   "outputs": [],
   "source": [
    "#Loading images - from Datacamp CNN course (cloud example)\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((56,56))\n",
    "])\n",
    "\n",
    "dataset_train = ImageFolder(\n",
    "    'Train',\n",
    "    transform = train_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5Mr9nTgLuwBa"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(dataset_train, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Nkdw0AbTfQ9"
   },
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "FTvmTS6RQB5T",
    "outputId": "a8839b4e-c49e-4d00-956f-8b107e0ab422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 56, 56])\n",
      "Label: tensor([32])\n",
      "tensor([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Trying to display images\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "image, label = next(iter(dataloader_train))\n",
    "print(image.shape)\n",
    "print('Label:',label)\n",
    "print(label)\n",
    "\n",
    "image = image.squeeze().permute(1,2,0)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:\n",
      "Class 0: 93.3786\n",
      "Class 1: 8.8331\n",
      "Class 10: 9.7560\n",
      "Class 11: 14.8557\n",
      "Class 12: 9.3379\n",
      "Class 13: 9.0785\n",
      "Class 14: 25.1404\n",
      "Class 15: 31.1262\n",
      "Class 16: 46.6893\n",
      "Class 17: 17.6662\n",
      "Class 18: 16.3412\n",
      "Class 19: 93.3786\n",
      "Class 2: 8.7153\n",
      "Class 20: 54.4708\n",
      "Class 21: 59.4227\n",
      "Class 22: 50.2808\n",
      "Class 23: 38.4500\n",
      "Class 24: 72.6278\n",
      "Class 25: 13.0730\n",
      "Class 26: 32.6825\n",
      "Class 27: 81.7062\n",
      "Class 28: 36.3139\n",
      "Class 29: 72.6278\n",
      "Class 3: 13.9074\n",
      "Class 30: 43.5767\n",
      "Class 31: 25.1404\n",
      "Class 32: 81.7062\n",
      "Class 33: 28.4608\n",
      "Class 34: 46.6893\n",
      "Class 35: 16.3412\n",
      "Class 36: 50.2808\n",
      "Class 37: 93.3786\n",
      "Class 38: 9.4732\n",
      "Class 39: 65.3650\n",
      "Class 4: 9.9038\n",
      "Class 40: 54.4708\n",
      "Class 41: 81.7062\n",
      "Class 42: 81.7062\n",
      "Class 5: 10.5427\n",
      "Class 6: 46.6893\n",
      "Class 7: 13.6177\n",
      "Class 8: 13.9074\n",
      "Class 9: 13.3398\n"
     ]
    }
   ],
   "source": [
    "num_samples = 39219\n",
    "num_images_per_class = {\n",
    "    0: 210, 1: 2220, 10: 2010, 11: 1320, 12: 2100, 13: 2160, 14: 780, 15: 630,\n",
    "    16: 420, 17: 1110, 18: 1200, 19: 210, 2: 2250, 20: 360, 21: 330, 22: 390,\n",
    "    23: 510, 24: 270, 25: 1500, 26: 600, 27: 240, 28: 540, 29: 270, 3: 1410,\n",
    "    30: 450, 31: 780, 32: 240, 33: 689, 34: 420, 35: 1200, 36: 390, 37: 210,\n",
    "    38: 2070, 39: 300, 4: 1980, 40: 360, 41: 240, 42: 240, 5: 1860, 6: 420,\n",
    "    7: 1440, 8: 1410, 9: 1470\n",
    "}\n",
    "\n",
    "class_weights = {}\n",
    "for class_id, num_images in num_images_per_class.items():\n",
    "    weight_for_class = num_samples / (num_images * 2)\n",
    "    class_weights[class_id] = weight_for_class\n",
    "\n",
    "print(\"Class Weights:\")\n",
    "for class_id, weight in class_weights.items():\n",
    "    print(f\"Class {class_id}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([\n",
    "    93.3786, 8.8331, 9.7560, 14.8557, 9.3379, 9.0785, 25.1404, 31.1262, 46.6893, 17.6662,\n",
    "    16.3412, 93.3786, 8.7153, 54.4708, 59.4227, 50.2808, 38.4500, 72.6278, 13.0730, 32.6825,\n",
    "    81.7062, 36.3139, 72.6278, 13.9074, 43.5767, 25.1404, 81.7062, 28.4608, 46.6893, 16.3412,\n",
    "    50.2808, 93.3786, 9.4732, 65.3650, 9.9038, 54.4708, 81.7062, 81.7062, 10.5427, 46.6893,\n",
    "    13.6177, 13.9074, 13.3398\n",
    "])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(weight= class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tuPa_BlLd52"
   },
   "source": [
    "## 1. Initial Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3GhuEKgdCoB"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9cpAerwGIiVu"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "configs = {\n",
    "    \"experiment_name\": \"first_experiment_CNN\",\n",
    "    \"data_augmentation\":True,\n",
    "\n",
    "    # Data\n",
    "    \"img_dimensions\" : (3,56,56),\n",
    "    \"batch_size\" : 350,\n",
    "    \"num_classes\" : 43,\n",
    "\n",
    "    # CNN\n",
    "    \"filter_sizes\" : [4, 16, 32, 64],\n",
    "    \"kernel_size\" : 5,\n",
    "    \"stride\": 2,\n",
    "    \"padding\":2,\n",
    "\n",
    "    #Optimzation\n",
    "    \"learning_rate\" : 0.0001,\n",
    "    \"epochs\" : 25,\n",
    "    \"weight_decay\" : 0.00001,\n",
    "    \"epochs\" : 35\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "d1kpFjNmuuGQ"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size = configs['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size = configs['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPPLsQP-eri7"
   },
   "source": [
    "## Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "xsDrW0b3JfNj"
   },
   "outputs": [],
   "source": [
    "if configs[\"data_augmentation\"]:\n",
    "  transform = transforms.Compose([\n",
    "      transforms.RandomHorizontalFlip(0.5),\n",
    "      transforms.Resize(56),\n",
    "      transforms.RandomRotation(45),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "else:\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(56),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhry9DmjA2Jj"
   },
   "source": [
    "### A. Pytorch Sequential MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMc_SdyFrsIu"
   },
   "source": [
    "#### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zLPfJJpqq0Ss"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "vptqybDBTW6T",
    "outputId": "015dea66-90e7-47f2-c6fb-7e15d6cd10dc"
   },
   "outputs": [],
   "source": [
    "if enable_wandb:\n",
    "  wandb.init(\n",
    "    project=\"Deciphering Traffic Signs\",\n",
    "    name=configs[\"experiment_name\"],\n",
    "    config=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0V4IvkHjPLP",
    "outputId": "a931da79-854b-4fd1-8672-bb6d561bb12f"
   },
   "outputs": [],
   "source": [
    "!pip install -U torchmetrics\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CE1TqqbMmY0U"
   },
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 250)\n",
    "        self.fc2 = nn.Linear(250, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "654NCINasXWO"
   },
   "outputs": [],
   "source": [
    "input_size = configs[\"img_dimensions\"][0] * configs[\"img_dimensions\"][1] * configs[\"img_dimensions\"][2]\n",
    "num_classes = configs[\"num_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-eTYZFbIn7B5"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMMOHh2Zn-Af",
    "outputId": "923fbb80-0a42-48b5-e720-1bc312545f9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "model                                    --\n",
       "├─Linear: 1-1                            2,352,250\n",
       "├─Linear: 1-2                            32,128\n",
       "├─Linear: 1-3                            5,547\n",
       "=================================================================\n",
       "Total params: 2,389,925\n",
       "Trainable params: 2,389,925\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model(input_size = input_size, num_classes = num_classes)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fVEJX-GeoDzF"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wk7Go1aBoGpW"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = configs['learning_rate'], weight_decay = configs['weight_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUj6Gb45oTSL"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Mf8I53QwtAoX"
   },
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Lx8u9f5joSs-"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader, model):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    acc = torchmetrics.Accuracy(task=\"MULTICLASS\", num_classes=configs['num_classes']).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, axis=1)\n",
    "            acc.update(preds, labels)\n",
    "\n",
    "    accuracy = acc.compute()\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QGjoXUIzof1E"
   },
   "outputs": [],
   "source": [
    "def get_loss(loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            if gpu_available and use_gpu:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(images)\n",
    "            loss = loss + criterion(outputs, labels)\n",
    "    return loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iG0mTxS-t8p8"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), \"./best_model.pt\")\n",
    "        elif validation_loss > self.min_validation_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "early_stopper = EarlyStopper(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1udopKNotV5",
    "outputId": "aaae0bae-7cd7-4b8d-ccf3-58ece5c51a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.761314630508423\n",
      "Epoch [1/35], Train Accuracy: 0.1475, Validation Accuracy: 0.1505\n",
      "Epoch 2  batch 1 . Training Loss:  3.694185256958008\n",
      "Epoch [2/35], Train Accuracy: 0.2078, Validation Accuracy: 0.2160\n",
      "Epoch 3  batch 1 . Training Loss:  3.634922504425049\n",
      "Epoch [3/35], Train Accuracy: 0.2098, Validation Accuracy: 0.2172\n",
      "Epoch 4  batch 1 . Training Loss:  3.626262664794922\n",
      "Epoch [4/35], Train Accuracy: 0.2392, Validation Accuracy: 0.2455\n",
      "Epoch 5  batch 1 . Training Loss:  3.563722848892212\n",
      "Epoch [5/35], Train Accuracy: 0.2610, Validation Accuracy: 0.2691\n",
      "Epoch 6  batch 1 . Training Loss:  3.5766592025756836\n",
      "Epoch [6/35], Train Accuracy: 0.2653, Validation Accuracy: 0.2752\n",
      "Epoch 7  batch 1 . Training Loss:  3.540334939956665\n",
      "Epoch [7/35], Train Accuracy: 0.2673, Validation Accuracy: 0.2771\n",
      "Epoch 8  batch 1 . Training Loss:  3.540482997894287\n",
      "Epoch [8/35], Train Accuracy: 0.2753, Validation Accuracy: 0.2839\n",
      "Epoch 9  batch 1 . Training Loss:  3.5625996589660645\n",
      "Epoch [9/35], Train Accuracy: 0.2843, Validation Accuracy: 0.2923\n",
      "Epoch 10  batch 1 . Training Loss:  3.5114378929138184\n",
      "Epoch [10/35], Train Accuracy: 0.2962, Validation Accuracy: 0.3067\n",
      "Epoch 11  batch 1 . Training Loss:  3.503263473510742\n",
      "Epoch [11/35], Train Accuracy: 0.3068, Validation Accuracy: 0.3146\n",
      "Epoch 12  batch 1 . Training Loss:  3.531609535217285\n",
      "Epoch [12/35], Train Accuracy: 0.3133, Validation Accuracy: 0.3214\n",
      "Epoch 13  batch 1 . Training Loss:  3.479691743850708\n",
      "Epoch [13/35], Train Accuracy: 0.3140, Validation Accuracy: 0.3215\n",
      "Epoch 14  batch 1 . Training Loss:  3.4622771739959717\n",
      "Epoch [14/35], Train Accuracy: 0.3258, Validation Accuracy: 0.3341\n",
      "Epoch 15  batch 1 . Training Loss:  3.468007802963257\n",
      "Epoch [15/35], Train Accuracy: 0.3316, Validation Accuracy: 0.3398\n",
      "Epoch 16  batch 1 . Training Loss:  3.5298900604248047\n",
      "Epoch [16/35], Train Accuracy: 0.3333, Validation Accuracy: 0.3408\n",
      "Epoch 17  batch 1 . Training Loss:  3.4715189933776855\n",
      "Epoch [17/35], Train Accuracy: 0.3364, Validation Accuracy: 0.3442\n",
      "Epoch 18  batch 1 . Training Loss:  3.4850990772247314\n",
      "Epoch [18/35], Train Accuracy: 0.3374, Validation Accuracy: 0.3437\n",
      "Epoch 19  batch 1 . Training Loss:  3.436643123626709\n",
      "Epoch [19/35], Train Accuracy: 0.3396, Validation Accuracy: 0.3464\n",
      "Epoch 20  batch 1 . Training Loss:  3.4536960124969482\n",
      "Epoch [20/35], Train Accuracy: 0.3607, Validation Accuracy: 0.3658\n",
      "Epoch 21  batch 1 . Training Loss:  3.453040599822998\n",
      "Epoch [21/35], Train Accuracy: 0.3630, Validation Accuracy: 0.3668\n",
      "Epoch 22  batch 1 . Training Loss:  3.5024166107177734\n",
      "Epoch [22/35], Train Accuracy: 0.3635, Validation Accuracy: 0.3670\n",
      "Epoch 23  batch 1 . Training Loss:  3.4834277629852295\n",
      "Epoch [23/35], Train Accuracy: 0.3698, Validation Accuracy: 0.3767\n",
      "Epoch 24  batch 1 . Training Loss:  3.437631130218506\n",
      "Epoch [24/35], Train Accuracy: 0.3754, Validation Accuracy: 0.3825\n",
      "Epoch 25  batch 1 . Training Loss:  3.406703472137451\n",
      "Epoch [25/35], Train Accuracy: 0.3786, Validation Accuracy: 0.3843\n",
      "Epoch 26  batch 1 . Training Loss:  3.4923202991485596\n",
      "Epoch [26/35], Train Accuracy: 0.3791, Validation Accuracy: 0.3826\n",
      "Epoch 27  batch 1 . Training Loss:  3.422691822052002\n",
      "Epoch [27/35], Train Accuracy: 0.3820, Validation Accuracy: 0.3871\n",
      "Epoch 28  batch 1 . Training Loss:  3.420776128768921\n",
      "Epoch [28/35], Train Accuracy: 0.3841, Validation Accuracy: 0.3881\n",
      "Epoch 29  batch 1 . Training Loss:  3.4788243770599365\n",
      "Epoch [29/35], Train Accuracy: 0.3849, Validation Accuracy: 0.3887\n",
      "Epoch 30  batch 1 . Training Loss:  3.4877243041992188\n",
      "Epoch [30/35], Train Accuracy: 0.3829, Validation Accuracy: 0.3855\n",
      "Epoch 31  batch 1 . Training Loss:  3.4195632934570312\n",
      "Epoch [31/35], Train Accuracy: 0.3867, Validation Accuracy: 0.3900\n",
      "Epoch 32  batch 1 . Training Loss:  3.395792007446289\n",
      "Epoch [32/35], Train Accuracy: 0.3869, Validation Accuracy: 0.3896\n",
      "Epoch 33  batch 1 . Training Loss:  3.4364707469940186\n",
      "Epoch [33/35], Train Accuracy: 0.3839, Validation Accuracy: 0.3841\n",
      "Epoch 34  batch 1 . Training Loss:  3.416938543319702\n",
      "Epoch [34/35], Train Accuracy: 0.3885, Validation Accuracy: 0.3934\n",
      "Epoch 35  batch 1 . Training Loss:  3.4219088554382324\n",
      "Epoch [35/35], Train Accuracy: 0.3890, Validation Accuracy: 0.3919\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(configs['epochs']):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if gpu_available and use_gpu:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            model = model.cuda()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch\", epoch + 1, \" batch\", i + 1, \". Training Loss: \", loss.item())\n",
    "            if enable_wandb:\n",
    "                wandb.log({\"loss\": loss})\n",
    "\n",
    "    # Calculate training and validation accuracy\n",
    "    train_acc = get_accuracy(train_loader, model)\n",
    "    val_acc = get_accuracy(val_loader, model)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{configs[\"epochs\"]}], Train Accuracy: {train_acc.item():.4f}, Validation Accuracy: {val_acc.item():.4f}')\n",
    "\n",
    "    if enable_wandb:\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_accuracy\": train_acc.item(), \"val_accuracy\": val_acc.item()})\n",
    "\n",
    "    # Calculate validation loss\n",
    "    validation_loss = get_loss(val_loader)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if early_stopper.early_stop(validation_loss):\n",
    "        print(\"Validation loss hasn't dropped. Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3f91ab1a0c445e9855a9fb2c1cc42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▅▄▄▄▄▄▃▃▄▃▂▂▄▂▃▂▂▂▃▃▂▁▃▂▁▃▃▁▁▂▁▂</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████████████</td></tr><tr><td>val_accuracy</td><td>▁▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>loss</td><td>3.42191</td></tr><tr><td>train_accuracy</td><td>0.38903</td></tr><tr><td>val_accuracy</td><td>0.39191</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">first_experiment_MLP</strong> at: <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/gxs8ukct' target=\"_blank\">https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/gxs8ukct</a><br/> View project at: <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20' target=\"_blank\">https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_072855-gxs8ukct/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if enable_wandb:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P56zhYo9d17j"
   },
   "source": [
    "### B. Pytoch CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ezyjp2jq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c57212f40f34ed38396d4f21c1c733d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>loss</td><td>█▇▆▅▄▃▃▄▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▃▁▁▂▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇█▇█▇█████</td></tr><tr><td>val_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▅▆▆▇▇▇▇▇▇█▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>28</td></tr><tr><td>loss</td><td>3.43662</td></tr><tr><td>train_accuracy</td><td>0.35543</td></tr><tr><td>val_accuracy</td><td>0.35901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">First_CNN_Model</strong> at: <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/ezyjp2jq' target=\"_blank\">https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/ezyjp2jq</a><br/> View project at: <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20' target=\"_blank\">https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_111039-ezyjp2jq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ezyjp2jq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55652575dc5b4b07a75ee50da6e4caf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112495655349145, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projectnb/ba865/students/jsale017/wandb/run-20240417_114659-8dwb4bfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/8dwb4bfr' target=\"_blank\">First_CNN_Model</a></strong> to <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20' target=\"_blank\">https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/8dwb4bfr' target=\"_blank\">https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/8dwb4bfr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/josesalerno/Deciphering%20Traffic%20Signs%20/runs/8dwb4bfr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14644130f5b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"Deciphering Traffic Signs\",\n",
    "    name=\"First_CNN_Model\",\n",
    "    config=configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "fqYvgbI5IYMh"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, img_dimensions, num_classes, filter_sizes, kernel_size, stride, padding):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(img_dimensions[0], filter_sizes[0], kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(img_dimensions[0], filter_sizes[0], kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(filter_sizes[0], filter_sizes[1], kernel_size, stride, padding)\n",
    "        self.conv3 = nn.Conv2d(filter_sizes[1], filter_sizes[2], kernel_size, stride, padding)\n",
    "        self.conv4 = nn.Conv2d(filter_sizes[2], filter_sizes[3], kernel_size, stride, padding)\n",
    "        self.pool = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(filter_sizes[0])\n",
    "        self.batchnorm2 = nn.BatchNorm2d(filter_sizes[1])\n",
    "        self.batchnorm3 = nn.BatchNorm2d(filter_sizes[2])\n",
    "        self.batchnorm4 = nn.BatchNorm2d(filter_sizes[3])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(filter_sizes[3], num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.batchnorm3(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.batchnorm4(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "        out = self.global_avg_pool(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "vazqNE3rycZ3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "CNN                                      --\n",
       "├─Conv2d: 1-1                            304\n",
       "├─ReLU: 1-2                              --\n",
       "├─Conv2d: 1-3                            1,616\n",
       "├─Conv2d: 1-4                            12,832\n",
       "├─Conv2d: 1-5                            51,264\n",
       "├─MaxPool2d: 1-6                         --\n",
       "├─BatchNorm2d: 1-7                       8\n",
       "├─BatchNorm2d: 1-8                       32\n",
       "├─BatchNorm2d: 1-9                       64\n",
       "├─BatchNorm2d: 1-10                      128\n",
       "├─Dropout: 1-11                          --\n",
       "├─AdaptiveAvgPool2d: 1-12                --\n",
       "├─Linear: 1-13                           2,795\n",
       "├─Softmax: 1-14                          --\n",
       "=================================================================\n",
       "Total params: 69,043\n",
       "Trainable params: 69,043\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CNN = CNN(configs['img_dimensions'], configs['num_classes'], configs['filter_sizes'], configs['kernel_size'], configs['stride'], configs['padding'])\n",
    "summary(model_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = model_CNN.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "q4pgp724yvdl"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "cDACIJs2yv8S"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_CNN.parameters(), lr = configs['learning_rate'], weight_decay = configs['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torchmetrics\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "JMgMAZucy0mv"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader, model):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    acc = torchmetrics.Accuracy(task=\"MULTICLASS\", num_classes=configs['num_classes']).to(device)\n",
    "\n",
    "    model_CNN.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "                \n",
    "            outputs = model_CNN(images)\n",
    "            preds = torch.argmax(outputs, axis=1)\n",
    "            acc.update(preds, labels)\n",
    "\n",
    "    accuracy = acc.compute()\n",
    "    model_CNN.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "aP7fKkKKy3T6"
   },
   "outputs": [],
   "source": [
    "def get_loss(loader):\n",
    "    model_CNN.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(loader):\n",
    "            if gpu_available and use_gpu:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model_CNN(images)\n",
    "            loss = loss + criterion(outputs, labels)\n",
    "    return loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "W0OIJ6GAy5Me"
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model_CNN.state_dict(), \"./best_model.pt\")\n",
    "        elif validation_loss > self.min_validation_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "early_stopper = EarlyStopper(patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peFk3vYKy8nP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  batch 1 . Training Loss:  3.7624692916870117\n",
      "Epoch [1/35], Train Accuracy: 0.1085, Validation Accuracy: 0.1050\n",
      "Epoch 2  batch 1 . Training Loss:  3.745945692062378\n",
      "Epoch [2/35], Train Accuracy: 0.1682, Validation Accuracy: 0.1519\n",
      "Epoch 3  batch 1 . Training Loss:  3.657038688659668\n",
      "Epoch [3/35], Train Accuracy: 0.2197, Validation Accuracy: 0.2042\n",
      "Epoch 4  batch 1 . Training Loss:  3.532409429550171\n",
      "Epoch [4/35], Train Accuracy: 0.2772, Validation Accuracy: 0.2595\n",
      "Epoch 5  batch 1 . Training Loss:  3.5738489627838135\n",
      "Epoch [5/35], Train Accuracy: 0.3076, Validation Accuracy: 0.2941\n",
      "Epoch 6  batch 1 . Training Loss:  3.5138700008392334\n",
      "Epoch [6/35], Train Accuracy: 0.3327, Validation Accuracy: 0.3186\n",
      "Epoch 7  batch 1 . Training Loss:  3.4428744316101074\n",
      "Epoch [7/35], Train Accuracy: 0.3439, Validation Accuracy: 0.3262\n",
      "Epoch 8  batch 1 . Training Loss:  3.4231317043304443\n",
      "Epoch [8/35], Train Accuracy: 0.3692, Validation Accuracy: 0.3515\n",
      "Epoch 9  batch 1 . Training Loss:  3.50309419631958\n",
      "Epoch [9/35], Train Accuracy: 0.3754, Validation Accuracy: 0.3607\n",
      "Epoch 10  batch 1 . Training Loss:  3.3981125354766846\n",
      "Epoch [10/35], Train Accuracy: 0.3852, Validation Accuracy: 0.3697\n",
      "Epoch 11  batch 1 . Training Loss:  3.4340717792510986\n",
      "Epoch [11/35], Train Accuracy: 0.3883, Validation Accuracy: 0.3738\n",
      "Epoch 12  batch 1 . Training Loss:  3.452211856842041\n",
      "Epoch [12/35], Train Accuracy: 0.4054, Validation Accuracy: 0.3901\n",
      "Epoch 13  batch 1 . Training Loss:  3.351360559463501\n",
      "Epoch [13/35], Train Accuracy: 0.4192, Validation Accuracy: 0.4020\n",
      "Epoch 14  batch 1 . Training Loss:  3.367687702178955\n",
      "Epoch [14/35], Train Accuracy: 0.4252, Validation Accuracy: 0.4057\n",
      "Epoch 15  batch 1 . Training Loss:  3.3884658813476562\n",
      "Epoch [15/35], Train Accuracy: 0.4317, Validation Accuracy: 0.4158\n",
      "Epoch 16  batch 1 . Training Loss:  3.365931510925293\n",
      "Epoch [16/35], Train Accuracy: 0.4407, Validation Accuracy: 0.4262\n",
      "Epoch 17  batch 1 . Training Loss:  3.36480712890625\n",
      "Epoch [17/35], Train Accuracy: 0.4485, Validation Accuracy: 0.4308\n",
      "Epoch 18  batch 1 . Training Loss:  3.347071647644043\n",
      "Epoch [18/35], Train Accuracy: 0.4563, Validation Accuracy: 0.4376\n",
      "Epoch 19  batch 1 . Training Loss:  3.3110716342926025\n",
      "Epoch [19/35], Train Accuracy: 0.4546, Validation Accuracy: 0.4353\n",
      "Epoch 20  batch 1 . Training Loss:  3.3145718574523926\n",
      "Epoch [22/35], Train Accuracy: 0.4804, Validation Accuracy: 0.4590\n",
      "Epoch 23  batch 1 . Training Loss:  3.3423893451690674\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(configs['epochs']):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if gpu_available and use_gpu:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            model = model_CNN.cuda()\n",
    "        outputs = model_CNN(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch\", epoch + 1, \" batch\", i + 1, \". Training Loss: \", loss.item())\n",
    "            if enable_wandb:\n",
    "                wandb.log({\"loss\": loss})\n",
    "\n",
    "    # Calculate training and validation accuracy\n",
    "    train_acc = get_accuracy(train_loader, model_CNN)\n",
    "    val_acc = get_accuracy(val_loader, model_CNN)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{configs[\"epochs\"]}], Train Accuracy: {train_acc.item():.4f}, Validation Accuracy: {val_acc.item():.4f}')\n",
    "\n",
    "    if enable_wandb:\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_accuracy\": train_acc.item(), \"val_accuracy\": val_acc.item()})\n",
    "\n",
    "    # Calculate validation loss\n",
    "    validation_loss = get_loss(val_loader)\n",
    "\n",
    "    # Check for early stopping\n",
    "    if early_stopper.early_stop(validation_loss):\n",
    "        print(\"Validation loss hasn't dropped. Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_wandb:\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Goom7I9-MaLm"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63E-98XlYngo"
   },
   "source": [
    "## Sources:\n",
    "- Generative AI was utilized for Debugging, code improvement, sentence structure and grammar.\n",
    "- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "- https://medium.com/@zergtant/use-weighted-loss-function-to-solve-imbalanced-data-classification-problems-749237f38b75 \n",
    "- https://thevatsalsaglani.medium.com/multi-class-image-classification-using-cnn-over-pytorch-and-the-basics-of-cnn-fdf425a11dc0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
